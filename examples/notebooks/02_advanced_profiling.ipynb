{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3d6ae2",
   "metadata": {},
   "source": [
    "# AMD AI Compute Observatory - Advanced Profiling\n",
    "\n",
    "This notebook covers advanced profiling techniques including kernel traces, GPU counters, and comparative analysis.\n",
    "\n",
    "Â© 2026 Sudheer Ibrahim Daniel Devu. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4b81e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aaco\n",
    "from aaco.core import Observatory\n",
    "from aaco.collectors import TimingCollector, CounterCollector, TraceCollector\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"AACO version: {aaco.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ff9d6",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "AACO supports various profiling configurations for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full profiling configuration\n",
    "full_config = {\n",
    "    \"profiling\": {\n",
    "        \"default_iterations\": 500,\n",
    "        \"default_warmup\": 50,\n",
    "        \"trace_level\": \"full\",  # api, kernel, or full\n",
    "    },\n",
    "    \"collectors\": {\n",
    "        \"timing\": {\n",
    "            \"enabled\": True,\n",
    "            \"resolution\": \"nanoseconds\"\n",
    "        },\n",
    "        \"memory\": {\n",
    "            \"enabled\": True,\n",
    "            \"track_gpu\": True,\n",
    "            \"track_cpu\": True\n",
    "        },\n",
    "        \"counters\": {\n",
    "            \"enabled\": True,\n",
    "            \"groups\": [\"compute\", \"memory\", \"cache\"]\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"statistics\": {\n",
    "            \"confidence_level\": 0.95,\n",
    "            \"outlier_detection\": \"iqr\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "obs = Observatory(config=full_config)\n",
    "print(\"Observatory configured for advanced profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fec6e6",
   "metadata": {},
   "source": [
    "## GPU Counter Profiling\n",
    "\n",
    "Collect hardware performance counters from the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513734c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available GPU counters\n",
    "available_counters = CounterCollector.available_counters()\n",
    "print(f\"Available counter groups: {list(available_counters.keys()) if isinstance(available_counters, dict) else available_counters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example counter profiling configuration\n",
    "counter_config = {\n",
    "    \"compute_counters\": [\n",
    "        \"GRBM_GUI_ACTIVE\",  # GPU active cycles\n",
    "        \"SQ_WAVES\",          # Shader waves\n",
    "        \"SQ_INSTS_VALU\",     # Vector ALU instructions\n",
    "    ],\n",
    "    \"memory_counters\": [\n",
    "        \"TCP_TCC_READ_REQ_sum\",  # Cache read requests\n",
    "        \"TCP_TCC_WRITE_REQ_sum\", # Cache write requests\n",
    "        \"TCC_HIT_sum\",           # L2 cache hits\n",
    "        \"TCC_MISS_sum\",          # L2 cache misses\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Counter configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b5f19",
   "metadata": {},
   "source": [
    "## Trace-Level Profiling\n",
    "\n",
    "Capture detailed execution traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b68797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure trace collection\n",
    "trace_config = {\n",
    "    \"trace_level\": \"full\",\n",
    "    \"output_format\": \"perfetto\",  # or \"json\", \"chrome\"\n",
    "    \"include_hip_api\": True,\n",
    "    \"include_kernel_launches\": True,\n",
    "    \"include_memory_operations\": True,\n",
    "}\n",
    "\n",
    "# Example trace collector usage\n",
    "# trace_collector = TraceCollector(**trace_config)\n",
    "print(\"Trace configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65db9f5",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Compare performance across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba85c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aaco.analytics import DriftDetector\n",
    "\n",
    "# Simulate baseline and current metrics\n",
    "np.random.seed(42)\n",
    "baseline_latencies = np.random.normal(10.0, 0.5, 100)\n",
    "np.random.seed(43)\n",
    "current_latencies = np.random.normal(10.5, 0.6, 100)  # Slight regression\n",
    "\n",
    "# Detect drift\n",
    "detector = DriftDetector(method=\"ewma_cusum\")\n",
    "result = detector.detect(baseline_latencies, current_latencies)\n",
    "\n",
    "print(f\"Drift detected: {result.has_drift}\")\n",
    "print(f\"Magnitude: {result.magnitude:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot comparison\n",
    "axes[0].boxplot(\n",
    "    [baseline_latencies, current_latencies],\n",
    "    labels=['Baseline', 'Current']\n",
    ")\n",
    "axes[0].set_ylabel('Latency (ms)')\n",
    "axes[0].set_title('Latency Comparison')\n",
    "\n",
    "# Distribution overlay\n",
    "axes[1].hist(baseline_latencies, bins=30, alpha=0.5, label='Baseline', color='blue')\n",
    "axes[1].hist(current_latencies, bins=30, alpha=0.5, label='Current', color='#ED1C24')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution Comparison')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679042dc",
   "metadata": {},
   "source": [
    "## Bottleneck Analysis\n",
    "\n",
    "Identify performance bottlenecks using metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1caacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aaco.analytics import BottleneckClassifier\n",
    "\n",
    "# Example metrics from a profiling session\n",
    "metrics = {\n",
    "    \"gpu_utilization\": 92.0,\n",
    "    \"sq_busy\": 0.88,\n",
    "    \"memory_bandwidth_utilization\": 0.35,\n",
    "    \"l2_hit_rate\": 0.92,\n",
    "}\n",
    "\n",
    "classifier = BottleneckClassifier()\n",
    "result = classifier.classify(metrics)\n",
    "\n",
    "print(f\"Workload Type: {result.category}\")\n",
    "print(f\"Confidence: {result.confidence:.2%}\")\n",
    "print(f\"\\nEvidence:\")\n",
    "for evidence in result.evidence:\n",
    "    print(f\"  - {evidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45399f94",
   "metadata": {},
   "source": [
    "## Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af21176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on bottleneck analysis\n",
    "recommendations = {\n",
    "    \"compute_bound\": [\n",
    "        \"Consider using mixed precision (FP16/BF16) for compute-intensive operations\",\n",
    "        \"Optimize kernel occupancy by adjusting workgroup sizes\",\n",
    "        \"Use kernel fusion to reduce launch overhead\",\n",
    "    ],\n",
    "    \"memory_bound\": [\n",
    "        \"Optimize memory access patterns for coalesced reads\",\n",
    "        \"Consider using shared memory for frequently accessed data\",\n",
    "        \"Profile memory bandwidth with roofline analysis\",\n",
    "    ],\n",
    "    \"launch_overhead_bound\": [\n",
    "        \"Use kernel batching or fusion\",\n",
    "        \"Implement HIP graphs for repetitive workloads\",\n",
    "        \"Reduce small kernel launches\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "if result.category in recommendations:\n",
    "    print(f\"\\nOptimization Recommendations for {result.category}:\")\n",
    "    for i, rec in enumerate(recommendations[result.category], 1):\n",
    "        print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb40b8c",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "Export profiling data in various formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Example export\n",
    "export_data = {\n",
    "    \"session_id\": \"demo_session\",\n",
    "    \"metrics\": metrics,\n",
    "    \"analysis\": {\n",
    "        \"bottleneck\": result.category,\n",
    "        \"confidence\": result.confidence,\n",
    "    },\n",
    "    \"baseline_stats\": {\n",
    "        \"mean\": float(np.mean(baseline_latencies)),\n",
    "        \"p95\": float(np.percentile(baseline_latencies, 95)),\n",
    "    },\n",
    "    \"current_stats\": {\n",
    "        \"mean\": float(np.mean(current_latencies)),\n",
    "        \"p95\": float(np.percentile(current_latencies, 95)),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"./analysis_results.json\")\n",
    "# output_path.write_text(json.dumps(export_data, indent=2))\n",
    "print(json.dumps(export_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89df84",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Learn about [Custom Collectors](03_custom_collectors.ipynb) for specialized profiling\n",
    "- Explore [Laboratory Mode](04_laboratory_mode.ipynb) for kernel-level analysis with eBPF"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
