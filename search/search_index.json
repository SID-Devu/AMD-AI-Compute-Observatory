{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AMD AI Compute Observatory","text":"![AACO Logo](assets/logo.png){ width=\"200\" }  **AACO-\u03a9\u221e | Model-to-Metal Performance Science &amp; Governance Engine**  [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge&amp;logo=github)](https://github.com/SID-Devu/AMD-AI-Compute-Observatory) [![License](https://img.shields.io/badge/License-Proprietary-red?style=for-the-badge)](../LICENSE) [![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&amp;logo=python)](https://python.org)"},{"location":"#what-is-aaco","title":"\ud83c\udfaf What is AACO?","text":"<p>AMD AI Compute Observatory (AACO) is a deterministic, self-calibrating, cross-layer AI performance laboratory designed for AMD hardware. It provides scientific measurement, reproducible truth, and automated governance for AI workloads.</p> <p>This is Performance Science</p> <p>AACO is not just another profiler. It's a complete Performance Science Infrastructure that answers questions with statistical rigor.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p> Deterministic Laboratory</p> <p>cgroups v2 isolation, CPU pinning, GPU clock lock for reproducible measurements</p> </li> <li> <p> Statistical Governance</p> <p>EWMA + CUSUM drift detection with robust baselines (median/MAD)</p> </li> <li> <p> Bayesian Root Cause</p> <p>Posterior probability ranking with evidence-based classification</p> </li> <li> <p>:material-gpu:{ .lg .middle } Hardware Digital Twin</p> <p>Microbenchmark-calibrated ceiling analysis with HEU scoring</p> </li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Install AACO\npip install aaco\n\n# Run your first profile\naaco profile --model resnet50.onnx --output results/\n\n# View the dashboard\naaco dashboard --session results/latest\n</code></pre>"},{"location":"#the-10-pillars","title":"\ud83d\udcca The 10 Pillars","text":"<p>AACO is built on 10 architectural pillars:</p> Pillar Name Description P1 Laboratory Mode Deterministic execution environment P2 eBPF Forensics Kernel-level interference detection P3 GPU Counter KFF Counter-calibrated kernel family fingerprinting P4 Attribution Probabilistic graph\u2192kernel mapping P5 Digital Twin Hardware-calibrated performance ceiling P6 Trace Lake Unified cross-layer trace storage P7 Governance Statistical regression detection P8 Root Cause Bayesian inference engine P9 Auto-Opt Hypothesis-driven optimization P10 Fleet Ops Multi-session trending"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li> <p> Installation</p> <p>Setup AACO on your system</p> </li> <li> <p> Quick Start</p> <p>Get started in 5 minutes</p> </li> <li> <p> User Guide</p> <p>Complete usage documentation</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation</p> </li> </ul>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>Proprietary Software</p> <p>This is proprietary source-available software. See LICENSE for terms.</p>   **\u00a9 2026 Sudheer Ibrahim Daniel Devu. All Rights Reserved.**"},{"location":"architecture/","title":"Architecture","text":"<p>AMD AI Compute Observatory Copyright \u00a9 2026 Sudheer Ibrahim Daniel Devu</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>AMD AI Compute Observatory (AACO) implements a layered architecture for comprehensive performance analysis of AI workloads on AMD Instinct accelerators.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                                 \u2502\n\u2502                        AMD AI COMPUTE OBSERVATORY                               \u2502\n\u2502                                                                                 \u2502\n\u2502                   Model-to-Metal Performance Analysis Platform                  \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTERFACE LAYER                                                                 \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502   CLI Tool    \u2502  \u2502  HTML Report  \u2502  \u2502   JSON API    \u2502  \u2502   Dashboard   \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502   aaco run    \u2502  \u2502  Interactive  \u2502  \u2502  Programmatic \u2502  \u2502   Streamlit   \u2502   \u2502\n\u2502  \u2502   aaco diff   \u2502  \u2502  Visualization\u2502  \u2502    Access     \u2502  \u2502   Real-time   \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 GOVERNANCE LAYER                                                                \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2502     Statistical     \u2502  \u2502     Root Cause      \u2502  \u2502       Fleet         \u2502     \u2502\n\u2502  \u2502     Regression      \u2502  \u2502      Analysis       \u2502  \u2502     Operations      \u2502     \u2502\n\u2502  \u2502     Governance      \u2502  \u2502       Engine        \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2502  \u2022 EWMA Detection   \u2502  \u2502  \u2022 Bayesian RCPP    \u2502  \u2502  \u2022 Multi-session    \u2502     \u2502\n\u2502  \u2502  \u2022 CUSUM Analysis   \u2502  \u2502  \u2022 Evidence Ranking \u2502  \u2502  \u2022 Trend Analysis   \u2502     \u2502\n\u2502  \u2502  \u2022 Robust Baseline  \u2502  \u2502  \u2022 Causal Inference \u2502  \u2502  \u2022 Fleet Heatmaps   \u2502     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTELLIGENCE LAYER                                                              \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502    Kernel     \u2502  \u2502 Probabilistic \u2502  \u2502   Hardware    \u2502  \u2502   Unified     \u2502   \u2502\n\u2502  \u2502  Fingerprint  \u2502  \u2502  Attribution  \u2502  \u2502    Digital    \u2502  \u2502    Trace      \u2502   \u2502\n\u2502  \u2502    Engine     \u2502  \u2502    Engine     \u2502  \u2502     Twin      \u2502  \u2502     Lake      \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502  \u2022 Family     \u2502  \u2502  \u2022 KAR Score  \u2502  \u2502  \u2022 HEU Score  \u2502  \u2502  \u2022 Parquet    \u2502   \u2502\n\u2502  \u2502    Classify   \u2502  \u2502  \u2022 PFI Score  \u2502  \u2502  \u2022 Microbench \u2502  \u2502  \u2022 Perfetto   \u2502   \u2502\n\u2502  \u2502  \u2022 Counter    \u2502  \u2502  \u2022 LTS Score  \u2502  \u2502  \u2022 Calibrate  \u2502  \u2502  \u2022 Cross-ref  \u2502   \u2502\n\u2502  \u2502    Signature  \u2502  \u2502  \u2022 Graph Map  \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 MEASUREMENT LAYER                                                               \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                                 \u2502  \u2502                                     \u2502 \u2502\n\u2502  \u2502        Laboratory Mode          \u2502  \u2502       eBPF Forensic Tracer          \u2502 \u2502\n\u2502  \u2502                                 \u2502  \u2502                                     \u2502 \u2502\n\u2502  \u2502  \u2022 cgroups v2 Isolation         \u2502  \u2502  \u2022 Scheduler Interference (SII)     \u2502 \u2502\n\u2502  \u2502  \u2022 CPU Core Pinning             \u2502  \u2502  \u2022 Fault Penalty Index (FPI)        \u2502 \u2502\n\u2502  \u2502  \u2022 NUMA Memory Binding          \u2502  \u2502  \u2022 Context Switch Tracking          \u2502 \u2502\n\u2502  \u2502  \u2022 GPU Clock Locking            \u2502  \u2502  \u2022 Cache Miss Events                \u2502 \u2502\n\u2502  \u2502  \u2022 IRQ Affinity Control         \u2502  \u2502                                     \u2502 \u2502\n\u2502  \u2502                                 \u2502  \u2502                                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 COLLECTION LAYER                                                                \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502  ORT Runner   \u2502  \u2502   rocprof     \u2502  \u2502    System     \u2502  \u2502     GPU       \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502   Wrapper     \u2502  \u2502    Sampler    \u2502  \u2502    Sampler    \u2502   \u2502\n\u2502  \u2502  \u2022 MIGraphX   \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502  \u2022 ROCm EP    \u2502  \u2502  \u2022 HIP Trace  \u2502  \u2502  \u2022 CPU Load   \u2502  \u2502  \u2022 Clocks     \u2502   \u2502\n\u2502  \u2502  \u2022 CUDA EP    \u2502  \u2502  \u2022 HSA Trace  \u2502  \u2502  \u2022 Memory     \u2502  \u2502  \u2022 Power      \u2502   \u2502\n\u2502  \u2502  \u2022 CPU EP     \u2502  \u2502  \u2022 Counters   \u2502  \u2502  \u2022 Context    \u2502  \u2502  \u2022 Temp       \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INFRASTRUCTURE LAYER                                                            \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2502   Session Manager   \u2502  \u2502     Data Schema     \u2502  \u2502      Utilities      \u2502     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2502  \u2022 Lifecycle        \u2502  \u2502  \u2022 Type-safe        \u2502  \u2502  \u2022 High-res timing  \u2502     \u2502\n\u2502  \u2502  \u2022 Artifact Store   \u2502  \u2502  \u2022 Dataclasses      \u2502  \u2502  \u2022 Subprocess mgmt  \u2502     \u2502\n\u2502  \u2502  \u2022 Environment      \u2502  \u2502  \u2022 Parquet Schema   \u2502  \u2502  \u2022 /proc readers    \u2502     \u2502\n\u2502  \u2502                     \u2502  \u2502                     \u2502  \u2502                     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 EXTERNAL DEPENDENCIES                                                           \u2502\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2502 ONNX Runtime  \u2502  \u2502    rocprof    \u2502  \u2502   rocm-smi    \u2502  \u2502    Linux      \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502    Kernel     \u2502   \u2502\n\u2502  \u2502  MIGraphX EP  \u2502  \u2502  GPU Profiler \u2502  \u2502   Telemetry   \u2502  \u2502  eBPF/proc    \u2502   \u2502\n\u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502  \u2502               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              DATA FLOW PIPELINE                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502             \u2502\n    \u2502 ONNX Model  \u2502\n    \u2502             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                      GRAPH ANALYSIS                              \u2502\n    \u2502                                                                  \u2502\n    \u2502   Model Loading \u2500\u2500\u25ba Node Extraction \u2500\u2500\u25ba Partition Mapping       \u2502\n    \u2502                                                                  \u2502\n    \u2502   Output: graph_nodes.parquet, graph_edges.parquet              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                   DETERMINISTIC EXECUTION                        \u2502\n    \u2502                                                                  \u2502\n    \u2502   Laboratory Setup \u2500\u2500\u25ba Warmup Phase \u2500\u2500\u25ba Measurement Phase       \u2502\n    \u2502                                                                  \u2502\n    \u2502   Output: inference_iters.parquet, inference_summary.json       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u25bc                   \u25bc                   \u25bc                  \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   rocprof   \u2502     \u2502   System    \u2502     \u2502    GPU      \u2502    \u2502    eBPF     \u2502\n    \u2502   Tracing   \u2502     \u2502   Sampling  \u2502     \u2502   Sampling  \u2502    \u2502   Tracing   \u2502\n    \u2502             \u2502     \u2502             \u2502     \u2502             \u2502    \u2502             \u2502\n    \u2502 Kernel exec \u2502     \u2502  CPU, Mem   \u2502     \u2502 Clocks, Pwr \u2502    \u2502  Scheduler  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502                   \u2502                  \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                      UNIFIED TRACE LAKE                          \u2502\n    \u2502                                                                  \u2502\n    \u2502   Parquet Storage \u2500\u2500\u25ba Cross-Reference \u2500\u2500\u25ba Perfetto Export       \u2502\n    \u2502                                                                  \u2502\n    \u2502   Schema: TimestampNs, EventType, Duration, Metadata            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     ATTRIBUTION ENGINE                           \u2502\n    \u2502                                                                  \u2502\n    \u2502   Graph\u2192Kernel Mapping \u2500\u2500\u25ba KAR/PFI/LTS Scoring \u2500\u2500\u25ba Grouping    \u2502\n    \u2502                                                                  \u2502\n    \u2502   Output: kernel_groups.parquet, op_to_kernel_map.parquet       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                   BOTTLENECK CLASSIFICATION                      \u2502\n    \u2502                                                                  \u2502\n    \u2502   Feature Extraction \u2500\u2500\u25ba Rule Engine \u2500\u2500\u25ba Confidence Scoring    \u2502\n    \u2502                                                                  \u2502\n    \u2502   Categories: Launch-bound, CPU-bound, Memory-bound,            \u2502\n    \u2502               Compute-bound, Thermal-throttled                   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                   STATISTICAL GOVERNANCE                         \u2502\n    \u2502                                                                  \u2502\n    \u2502   EWMA Detection \u2500\u2500\u25ba CUSUM Analysis \u2500\u2500\u25ba Baseline Comparison     \u2502\n    \u2502                                                                  \u2502\n    \u2502   Output: drift_status, change_points, regression_verdict       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                    ROOT CAUSE ANALYSIS                           \u2502\n    \u2502                                                                  \u2502\n    \u2502   Evidence Collection \u2500\u2500\u25ba Bayesian Inference \u2500\u2500\u25ba RCPP Ranking  \u2502\n    \u2502                                                                  \u2502\n    \u2502   Output: suspected_cause, posterior_probability, evidence      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                    REPORT GENERATION                             \u2502\n    \u2502                                                                  \u2502\n    \u2502   Template Rendering \u2500\u2500\u25ba Visualization \u2500\u2500\u25ba Export               \u2502\n    \u2502                                                                  \u2502\n    \u2502   Formats: HTML, JSON, Terminal, Perfetto                       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#module-structure","title":"Module Structure","text":"<pre><code>aaco/\n\u251c\u2500\u2500 core/                    # Infrastructure and session management\n\u2502   \u251c\u2500\u2500 session.py           # Session lifecycle management\n\u2502   \u251c\u2500\u2500 schema.py            # Data structures and type definitions\n\u2502   \u2514\u2500\u2500 utils.py             # Common utilities\n\u2502\n\u251c\u2500\u2500 collectors/              # Data collection subsystem\n\u2502   \u251c\u2500\u2500 rocm_smi_sampler.py  # GPU telemetry collection\n\u2502   \u251c\u2500\u2500 sys_sampler.py       # System metrics collection\n\u2502   \u251c\u2500\u2500 clocks.py            # Clock management\n\u2502   \u2514\u2500\u2500 driver_interface.py  # Low-level driver access\n\u2502\n\u251c\u2500\u2500 profiler/                # GPU profiling subsystem\n\u2502   \u2514\u2500\u2500 rocprof_wrapper.py   # rocprof integration\n\u2502\n\u251c\u2500\u2500 analytics/               # Analysis and diagnostics\n\u2502   \u251c\u2500\u2500 classify.py          # Bottleneck classification\n\u2502   \u251c\u2500\u2500 metrics.py           # Derived metrics computation\n\u2502   \u251c\u2500\u2500 attribution.py       # Graph-to-kernel attribution\n\u2502   \u251c\u2500\u2500 root_cause.py        # Bayesian root cause analysis\n\u2502   \u251c\u2500\u2500 regression_guard.py  # Statistical regression detection\n\u2502   \u2514\u2500\u2500 recommendation_engine.py\n\u2502\n\u251c\u2500\u2500 governance/              # Fleet and regression governance\n\u251c\u2500\u2500 laboratory/              # Deterministic execution environment\n\u251c\u2500\u2500 calibration/             # Hardware calibration subsystem\n\u251c\u2500\u2500 tracelake/               # Unified trace storage\n\u251c\u2500\u2500 report/                  # Report generation\n\u251c\u2500\u2500 dashboard/               # Real-time monitoring\n\u2514\u2500\u2500 cli.py                   # Command-line interface\n</code></pre>"},{"location":"architecture/#key-design-principles","title":"Key Design Principles","text":""},{"location":"architecture/#deterministic-measurement","title":"Deterministic Measurement","text":"<p>All measurements are designed for reproducibility: - Configurable warmup iterations to reach steady state - Statistical aggregation over measurement iterations - Environment capture for reproducibility verification - Isolation mechanisms to minimize interference</p>"},{"location":"architecture/#cross-layer-attribution","title":"Cross-Layer Attribution","text":"<p>Performance data is correlated across execution layers: - ONNX graph nodes map to MIGraphX partitions - Partitions map to HIP kernel launches - Kernel execution correlates with hardware counters - System events provide context for anomalies</p>"},{"location":"architecture/#statistical-rigor","title":"Statistical Rigor","text":"<p>All comparisons use statistically sound methods: - Robust baseline computation using median/MAD - EWMA for drift detection with configurable sensitivity - CUSUM for change point detection - Confidence intervals for all reported metrics</p>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<p>The architecture supports extension: - Pluggable collectors for new data sources - Modular analytics for new classification rules - Template-based reporting for custom formats - API access for integration with external systems</p>"},{"location":"architecture/#hardware-requirements","title":"Hardware Requirements","text":"Component Minimum Recommended GPU AMD Instinct MI100 AMD Instinct MI300X ROCm 5.6+ 6.0+ Memory 32GB 64GB+ Storage SSD 100GB NVMe 500GB+"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":"Operation Typical Duration Notes Model load 1-10s Depends on model size Warmup 5-30s Configurable iterations Measurement 10-60s Configurable iterations rocprof trace +20-50% overhead Profiling adds overhead Report generation 1-5s Depends on data volume"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Methodology - Measurement methodology details</li> <li>Data Schema - Complete data schema reference</li> <li>Bottleneck Taxonomy - Classification rules</li> </ul>"},{"location":"bottleneck_taxonomy/","title":"Bottleneck Taxonomy","text":""},{"location":"bottleneck_taxonomy/#overview","title":"Overview","text":"<p>This document defines the complete taxonomy of performance bottlenecks that AACO can detect and classify. Understanding these categories is essential for systematic performance optimization.</p>"},{"location":"bottleneck_taxonomy/#bottleneck-categories","title":"Bottleneck Categories","text":""},{"location":"bottleneck_taxonomy/#1-compute-bound","title":"1. Compute Bound","text":"<p>Definition: The GPU compute units are the limiting factor; work is genuinely compute-intensive.</p> <p>Indicators: - GPU utilization &gt; 85% - GPU Active Ratio &gt; 0.7 - Low memory bandwidth utilization - Kernels have high arithmetic intensity</p> <p>Evidence: <pre><code>\u2022 High GPU utilization: 92%\n\u2022 High GPU active ratio: 0.85\n\u2022 Memory bandwidth not saturated\n</code></pre></p> <p>Optimizations: - FP16/INT8 quantization (reduce compute requirements) - Algorithm optimization (fewer FLOPS) - Kernel fusion (reduce overhead) - Consider tensor cores if available</p> <p>This is actually good: Being compute-bound means you're using the hardware efficiently. Optimization focuses on reducing work, not fixing inefficiencies.</p>"},{"location":"bottleneck_taxonomy/#2-memory-bound","title":"2. Memory Bound","text":"<p>Definition: Memory bandwidth is the limiting factor; compute units wait for data.</p> <p>Indicators: - High memory bandwidth utilization (&gt;80%) - Low GPU utilization (&lt;50%) - Large tensor operations - High VRAM traffic</p> <p>Evidence: <pre><code>\u2022 High memory utilization: 88%\n\u2022 Low GPU compute utilization: 45%\n\u2022 Large intermediate tensors detected\n</code></pre></p> <p>Optimizations: - Operator fusion (reduce memory traffic) - In-place operations where possible - Smaller data types (FP16 reduces bandwidth 2x) - Optimize memory access patterns - Layer reordering to improve locality</p>"},{"location":"bottleneck_taxonomy/#3-launch-overhead","title":"3. Launch Overhead","text":"<p>Definition: Too many small GPU kernel launches; launch latency dominates.</p> <p>Indicators: - Microkernel % &gt; 30% - Kernel Amplification Ratio &gt; 10x - GPU Active Ratio &lt; 0.3 - High launch rate (&gt;5000 kernels/sec)</p> <p>Evidence: <pre><code>\u2022 High microkernel %: 45%\n\u2022 High kernel amplification: 15x\n\u2022 Low GPU active ratio: 0.25\n\u2022 Launch rate: 8000 kernels/sec\n</code></pre></p> <p>Why this happens: - Elementwise operations spawn separate kernels - Lack of operator fusion - Suboptimal graph optimization - Many small operations in model architecture</p> <p>Optimizations: - Enable aggressive operator fusion in the EP - Use MIGraphX EP's fusion capabilities - Review ONNX graph for optimization opportunities - Consider graph-level optimizations (e.g., opset version) - Batch small operations where possible</p>"},{"location":"bottleneck_taxonomy/#4-data-transfer","title":"4. Data Transfer","text":"<p>Definition: Host-device memory transfers are the bottleneck.</p> <p>Indicators: - Low GPU utilization - High PCIe activity - Large input/output tensors on host - Frequent synchronization points</p> <p>Evidence: <pre><code>\u2022 Low GPU utilization during inference\n\u2022 Large tensors copied per iteration\n\u2022 Multiple D2H/H2D transfers detected\n</code></pre></p> <p>Optimizations: - Pin host memory for faster transfers - Use async transfers with pipelining - Keep tensors on device between operations - Reduce data movement in application code</p>"},{"location":"bottleneck_taxonomy/#5-cpu-bound","title":"5. CPU Bound","text":"<p>Definition: Host CPU processing limits GPU utilization.</p> <p>Indicators: - High CPU utilization (&gt;80%) - Low GPU utilization (&lt;50%) - Single-threaded pre/post processing - Inefficient data preparation</p> <p>Evidence: <pre><code>\u2022 High CPU utilization: 95%\n\u2022 Low GPU utilization: 35%\n\u2022 CPU appears to be limiting GPU work submission\n</code></pre></p> <p>Optimizations: - Profile CPU code for hotspots - Parallelize data preprocessing - Use async inference APIs - Move preprocessing to GPU if possible - Optimize Python code (NumPy vectorization, Cython)</p>"},{"location":"bottleneck_taxonomy/#6-thermal-throttle","title":"6. Thermal Throttle","text":"<p>Definition: GPU reducing performance due to thermal limits.</p> <p>Indicators: - Temperature &gt; 85\u00b0C (junction) - Clock frequency reduction observed - Performance degradation over time - Power limit reached</p> <p>Evidence: <pre><code>\u2022 High temperature: 92\u00b0C\n\u2022 Clock frequency dropped from 2100 to 1800 MHz\n\u2022 Performance degraded 15% over 5 minutes\n</code></pre></p> <p>Optimizations: - Improve cooling (better thermal paste, airflow) - Reduce ambient temperature - Set conservative power limits - Add pauses between benchmark iterations - Consider undervolting (if supported)</p>"},{"location":"bottleneck_taxonomy/#7-frequency-scaling","title":"7. Frequency Scaling","text":"<p>Definition: Clock frequency instability affecting performance consistency.</p> <p>Indicators: - Large clock frequency variation (&gt;200 MHz range) - Power management enabled - Non-performance governor - Inconsistent per-iteration latencies</p> <p>Evidence: <pre><code>\u2022 Clock variation: 1600-2100 MHz during benchmark\n\u2022 Scaling governor: ondemand (not performance)\n\u2022 High latency coefficient of variation: 25%\n</code></pre></p> <p>Optimizations: - Lock GPU clocks: <code>rocm-smi --setperflevel high</code> - Set performance governor: <code>cpupower frequency-set -g performance</code> - Disable power management during benchmarks - Set fixed clock frequencies</p>"},{"location":"bottleneck_taxonomy/#8-warmup-instability","title":"8. Warmup Instability","text":"<p>Definition: Performance differs between warmup and measurement phases.</p> <p>Indicators: - Warmup latency &gt; 20% higher than measurement - First few iterations significantly slower - JIT compilation detected - Memory allocation spikes during warmup</p> <p>Evidence: <pre><code>\u2022 Warmup mean: 15.2ms, Measurement mean: 12.1ms\n\u2022 Warmup effect: +25%\n\u2022 High variance in early iterations\n</code></pre></p> <p>Causes: - JIT compilation (first-time kernel compilation) - Lazy memory allocation - Cache warming - GPU power state transition</p> <p>Optimizations: - Increase warmup iterations - Pre-compile kernels if supported - Use persistent execution modes - Pre-allocate memory pools</p>"},{"location":"bottleneck_taxonomy/#9-kernel-fragmentation","title":"9. Kernel Fragmentation","text":"<p>Definition: Many unique kernels with poor reuse.</p> <p>Indicators: - High unique kernel count - Low kernel reuse ratio - Many kernels with single call - Dynamic shapes causing kernel regeneration</p> <p>Evidence: <pre><code>\u2022 500 unique kernels in single inference\n\u2022 Average calls per kernel: 1.2\n\u2022 Many dynamically shaped operations\n</code></pre></p> <p>Optimizations: - Use static shapes where possible - Enable kernel caching - Group similar operations - Review model architecture for regularization</p>"},{"location":"bottleneck_taxonomy/#10-balanced-no-bottleneck","title":"10. Balanced (No Bottleneck)","text":"<p>Definition: No single dominant bottleneck; system is reasonably optimized.</p> <p>Indicators: - Moderate GPU utilization (50-80%) - Reasonable GPU Active Ratio (0.5-0.8) - Low microkernel percentage - Stable latencies</p> <p>Evidence: <pre><code>\u2022 GPU utilization: 72%\n\u2022 GPU Active Ratio: 0.65\n\u2022 Microkernel %: 8%\n\u2022 Performance is consistent\n</code></pre></p> <p>Next Steps: - System is well-balanced - Further optimization requires architecture changes - Consider model distillation for further gains - Hardware upgrade for substantial improvement</p>"},{"location":"bottleneck_taxonomy/#classification-logic","title":"Classification Logic","text":"<pre><code>Priority Order:\n1. Thermal Throttle (if temp &gt; 85\u00b0C)\n2. Launch Overhead (if microkernel% &gt; 30% AND GPU active &lt; 0.4)\n3. Memory Bound (if mem_util &gt; 80% AND gpu_util &lt; 60%)\n4. Compute Bound (if gpu_util &gt; 85%)\n5. CPU Bound (if cpu_util &gt; 80% AND gpu_util &lt; 50%)\n6. Frequency Scaling (if clock_range &gt; 200MHz)\n7. Warmup Instability (if warmup_effect &gt; 20%)\n8. Balanced (default)\n</code></pre>"},{"location":"bottleneck_taxonomy/#bottleneck-combinations","title":"Bottleneck Combinations","text":"<p>Often multiple bottlenecks coexist. AACO reports: - Primary: Most impactful bottleneck - Secondary: Contributing factors</p> <p>Common combinations: - Launch Overhead + CPU Bound: Python preprocessing + many small kernels - Memory Bound + Thermal: Large tensors causing high power draw - Compute Bound + Thermal: Sustained heavy computation</p>"},{"location":"data_schema/","title":"Data Schema Reference","text":""},{"location":"data_schema/#overview","title":"Overview","text":"<p>All AACO data is strongly typed using Python dataclasses. This document provides the complete schema reference.</p>"},{"location":"data_schema/#session-metadata","title":"Session Metadata","text":"<pre><code>@dataclass\nclass SessionMetadata:\n    session_id: str          # Unique identifier (timestamp_uuid)\n    timestamp: str           # ISO 8601 timestamp\n    tag: Optional[str]       # User-provided tag for identification\n    hostname: str            # Machine hostname\n    platform: str            # OS platform string\n    python_version: str      # Python version\n    aaco_version: str        # AACO version\n    gpu: Dict[str, Any]      # GPU information dict\n    model_path: Optional[str] # Path to model being benchmarked\n    backend: Optional[str]   # Execution provider used\n    config: Dict[str, Any]   # Runtime configuration\n</code></pre>"},{"location":"data_schema/#inference-results","title":"Inference Results","text":"<pre><code>@dataclass\nclass InferenceResult:\n    iteration: int           # Iteration number (0-indexed)\n    phase: str              # \"warmup\" or \"measurement\"\n    latency_ms: float       # Per-iteration latency in milliseconds\n    t_start_ns: int         # Start timestamp (monotonic ns)\n    t_end_ns: int           # End timestamp (monotonic ns)\n</code></pre>"},{"location":"data_schema/#kernel-metrics","title":"Kernel Metrics","text":"<pre><code>@dataclass\nclass KernelExecution:\n    t_start_ns: int         # Kernel start time\n    t_end_ns: int           # Kernel end time\n    dur_ns: int             # Duration in nanoseconds\n    kernel_name: str        # Full kernel name\n    queue_id: int           # GPU queue ID\n\n@dataclass\nclass KernelSummary:\n    kernel_name: str        # Kernel name\n    calls: int              # Number of invocations\n    total_time_ms: float    # Total time across all calls\n    avg_time_us: float      # Average per-call time\n    min_time_us: float      # Minimum call time\n    max_time_us: float      # Maximum call time\n    std_time_us: float      # Standard deviation\n    pct_total: float        # Percentage of total kernel time\n\n@dataclass\nclass KernelMetrics:\n    total_kernel_count: int        # Total kernel launches\n    unique_kernel_count: int       # Unique kernel names\n    total_kernel_time_ms: float    # Sum of all kernel times\n    avg_kernel_duration_us: float  # Average kernel duration\n    microkernel_count: int         # Kernels &lt; threshold\n    microkernel_pct: float         # Percentage of microkernels\n    microkernel_threshold_us: float # Threshold (default 10\u03bcs)\n    launch_rate_per_sec: float     # Kernel launches per second\n    launch_tax_score: float        # Combined launch overhead score\n    kernel_amplification_ratio: float # Kernels per ONNX node\n    gpu_active_ratio: float        # Kernel time / wall time\n    top_kernels: List[KernelSummary] # Top N kernels by time\n</code></pre>"},{"location":"data_schema/#telemetry-samples","title":"Telemetry Samples","text":"<pre><code>@dataclass\nclass SystemSample:\n    timestamp_ns: int       # Sample timestamp\n    cpu_pct: Optional[float] # CPU utilization percentage\n    rss_mb: Optional[float]  # Resident set size (MB)\n    ctx_switches: Optional[int] # Context switches\n    page_faults: Optional[int]  # Page faults\n    load_avg_1m: Optional[float] # 1-minute load average\n\n@dataclass\nclass GPUSample:\n    timestamp_ns: int           # Sample timestamp\n    device_id: int              # GPU device index\n    gpu_util_pct: Optional[float]  # GPU utilization %\n    mem_util_pct: Optional[float]  # Memory utilization %\n    temp_c: Optional[float]        # Temperature Celsius\n    power_w: Optional[float]       # Power consumption Watts\n    sclk_mhz: Optional[int]        # GPU clock MHz\n    mclk_mhz: Optional[int]        # Memory clock MHz\n    vram_used_mb: Optional[float]  # VRAM used MB\n    vram_total_mb: Optional[float] # VRAM total MB\n</code></pre>"},{"location":"data_schema/#phase-metrics","title":"Phase Metrics","text":"<pre><code>@dataclass\nclass PhaseMetrics:\n    name: str               # Phase name (\"warmup\" or \"measurement\")\n    iterations: int         # Number of iterations\n    total_time_ms: float    # Total phase time\n    mean_ms: float          # Mean latency\n    std_ms: float           # Standard deviation\n    p50_ms: float           # Median (50th percentile)\n    p90_ms: float           # 90th percentile\n    p99_ms: float           # 99th percentile\n    min_ms: float           # Minimum latency\n    max_ms: float           # Maximum latency\n    iqr_ms: float           # Interquartile range\n    cov_pct: float          # Coefficient of variation (%)\n</code></pre>"},{"location":"data_schema/#derived-metrics","title":"Derived Metrics","text":"<pre><code>@dataclass\nclass DerivedMetrics:\n    warmup_phase: PhaseMetrics      # Warmup statistics\n    measurement_phase: PhaseMetrics # Measurement statistics\n    throughput: Dict[str, float]    # Throughput metrics\n    efficiency: Dict[str, float]    # Efficiency metrics\n    latency: Dict[str, float]       # Aggregate latency stats\n    system: Dict[str, float]        # System utilization\n    gpu: Dict[str, float]           # GPU utilization\n</code></pre>"},{"location":"data_schema/#bottleneck-classification","title":"Bottleneck Classification","text":"<pre><code>@dataclass\nclass BottleneckClassification:\n    primary: str                    # Primary bottleneck category\n    secondary: List[str]            # Secondary factors\n    confidence: float               # Classification confidence (0-1)\n    indicators: Dict[str, float]    # Indicator values used\n    evidence: List[str]             # Human-readable evidence strings\n    recommendations: List[str]      # Optimization suggestions\n</code></pre>"},{"location":"data_schema/#regression-verdict","title":"Regression Verdict","text":"<pre><code>@dataclass\nclass RegressionVerdict:\n    verdict: str                    # \"REGRESSION\", \"IMPROVEMENT\", \"NEUTRAL\"\n    confidence: float               # Verdict confidence (0-1)\n    regressions: List[str]          # Metrics that regressed\n    improvements: List[str]         # Metrics that improved\n    comparisons: List[Dict]         # Per-metric comparisons\n    p_value: Optional[float]        # Statistical significance\n    summary: str                    # Human-readable summary\n</code></pre>"},{"location":"data_schema/#json-serialization","title":"JSON Serialization","text":"<p>All schema objects serialize to JSON via <code>__dict__</code>:</p> <pre><code># Writing\nwith open(\"inference_results.json\", \"w\") as f:\n    json.dump([r.__dict__ for r in results], f, indent=2)\n\n# Reading\nwith open(\"inference_results.json\") as f:\n    data = json.load(f)\n    results = [InferenceResult(**d) for d in data]\n</code></pre>"},{"location":"data_schema/#parquet-schema-future","title":"Parquet Schema (Future)","text":"<p>For analytics at scale, results export to Parquet with these columns:</p> <pre><code>inference_results.parquet:\n  - session_id: string\n  - iteration: int64\n  - phase: string (categorical)\n  - latency_ms: float64\n  - t_start_ns: int64\n  - t_end_ns: int64\n\nkernel_traces.parquet:\n  - session_id: string\n  - kernel_name: string\n  - t_start_ns: int64\n  - t_end_ns: int64\n  - dur_ns: int64\n  - queue_id: int32\n\ntelemetry.parquet:\n  - session_id: string\n  - timestamp_ns: int64\n  - source: string (categorical: \"system\", \"gpu\")\n  - metric: string\n  - value: float64\n</code></pre>"},{"location":"methodology/","title":"AACO Methodology","text":""},{"location":"methodology/#performance-measurement-philosophy","title":"Performance Measurement Philosophy","text":""},{"location":"methodology/#principles","title":"Principles","text":"<ol> <li> <p>Session-Based Evidence: Every benchmark run produces a complete, self-contained \"evidence bundle\" that captures all context needed to understand and reproduce results.</p> </li> <li> <p>Wall Clock Reality: We measure what matters - end-to-end latency as experienced by applications - while also capturing the decomposition needed for optimization.</p> </li> <li> <p>Statistical Rigor: Performance measurements are inherently noisy. We use appropriate statistical methods (percentiles, confidence intervals, significance tests) rather than single-point comparisons.</p> </li> <li> <p>Full-Stack Visibility: Performance issues can arise at any layer. AACO instruments from kernel launches through inference APIs to application-level timing.</p> </li> </ol>"},{"location":"methodology/#measurement-phases","title":"Measurement Phases","text":""},{"location":"methodology/#warmup-phase","title":"Warmup Phase","text":"<p>Purpose: Allow the system to reach steady state before measurement.</p> <p>What happens during warmup: - JIT compilation completes - Memory pools are allocated - GPU caches warm up - Frequency governors stabilize</p> <p>Configuration: Default 10 iterations, adjustable via <code>--warmup</code></p> <p>Key Insight: Compare warmup vs measurement to detect: - JIT compilation overhead - Memory allocation delays - Lazy initialization costs</p>"},{"location":"methodology/#measurement-phase","title":"Measurement Phase","text":"<p>Purpose: Capture representative steady-state performance.</p> <p>Methodology: - Run N iterations (default 100) - Record per-iteration latency with high-resolution timer - Compute statistical aggregates (mean, std, percentiles)</p> <p>Why per-iteration matters: - Detects outliers and tail latency - Reveals performance variability - Enables statistical comparison</p>"},{"location":"methodology/#key-metrics","title":"Key Metrics","text":""},{"location":"methodology/#latency-metrics","title":"Latency Metrics","text":"Metric Definition Use Case Mean Arithmetic average General comparison Median (P50) 50<sup>th</sup> percentile Typical user experience P90/P99 90<sup>th</sup>/99<sup>th</sup> percentile Tail latency (SLA) Std Dev Standard deviation Consistency CoV Std/Mean \u00d7 100% Normalized variability IQR P75 - P25 Robust spread measure"},{"location":"methodology/#efficiency-metrics","title":"Efficiency Metrics","text":""},{"location":"methodology/#gpu-active-ratio","title":"GPU Active Ratio","text":"<pre><code>GPU Active Ratio = Total Kernel Time / Wall Clock Time\n</code></pre> <p>Interpretation: - 1.0 = GPU is fully utilized (100% of time in kernels) - 0.5 = GPU is idle half the time - Low values indicate launch overhead, data transfer, or CPU bottlenecks</p>"},{"location":"methodology/#kernel-amplification-ratio-kar","title":"Kernel Amplification Ratio (KAR)","text":"<pre><code>KAR = Total GPU Kernel Count / ONNX Node Count\n</code></pre> <p>Interpretation: - 1.0 = Perfect 1:1 mapping (ideal) - 10.0 = Each ONNX node spawns 10 kernels (fusion opportunity) - High KAR + many microkernels = launch overhead bottleneck</p>"},{"location":"methodology/#microkernel-percentage","title":"Microkernel Percentage","text":"<pre><code>Microkernel % = (Kernels &lt; 10\u03bcs) / Total Kernels \u00d7 100%\n</code></pre> <p>Interpretation: - &lt;10% = Healthy (mostly substantial kernels) - &gt;30% = Launch overhead concern - &gt;50% = Severe fragmentation (optimization critical)</p>"},{"location":"methodology/#bottleneck-classification","title":"Bottleneck Classification","text":""},{"location":"methodology/#taxonomy","title":"Taxonomy","text":"Category Indicators Root Cause Compute Bound High GPU util (&gt;85%), High GPU Active Ratio Kernels are compute-intensive Memory Bound High mem util, Low GPU util Bandwidth-limited kernels Launch Overhead High microkernel %, High KAR, Low GPU Active Ratio Too many small kernels CPU Bound High CPU util, Low GPU util Host code limiting GPU Thermal Throttle High temp (&gt;85\u00b0C), Clock variation Power/cooling limits Data Transfer Low GPU util, PCIe activity Host-device copies"},{"location":"methodology/#evidence-based-classification","title":"Evidence-Based Classification","text":"<p>AACO uses weighted indicator scoring:</p> <pre><code>score = 0\nif microkernel_pct &gt; 30:\n    score += 0.4\n    evidence.append(\"High microkernel %\")\nif kar &gt; 10:\n    score += 0.3\n    evidence.append(\"High kernel amplification\")\nif gpu_active_ratio &lt; 0.3:\n    score += 0.3\n    evidence.append(\"Low GPU active ratio\")\n\nif score &gt; 0.6:\n    classification = \"LAUNCH_OVERHEAD\"\n</code></pre>"},{"location":"methodology/#regression-detection","title":"Regression Detection","text":""},{"location":"methodology/#statistical-methodology","title":"Statistical Methodology","text":"<ol> <li>Threshold-Based: Simple percentage comparison</li> <li>Regression: &gt;5% slower</li> <li>Improvement: &gt;5% faster</li> <li> <p>Neutral: Within 5%</p> </li> <li> <p>Statistical Significance: Welch's t-test</p> </li> <li>Compare latency distributions</li> <li>Account for unequal variances</li> <li> <p>Report p-value</p> </li> <li> <p>Combined Verdict:</p> </li> <li>Use both threshold and significance</li> <li>Higher confidence when statistically significant</li> <li>Conservative default (avoid false positives)</li> </ol>"},{"location":"methodology/#comparison-dimensions","title":"Comparison Dimensions","text":"Metric Higher is Better Threshold Mean latency No 5% P99 latency No 10% Throughput Yes 5% GPU Active Ratio Yes 5%"},{"location":"methodology/#best-practices","title":"Best Practices","text":""},{"location":"methodology/#reproducible-benchmarks","title":"Reproducible Benchmarks","text":"<ol> <li>Lock clocks: Use performance governor, fixed frequencies</li> <li>Isolate system: Minimize background processes</li> <li>Warm up: Sufficient warmup to reach steady state</li> <li>Repeat: Multiple sessions for confidence</li> </ol>"},{"location":"methodology/#environment-capture","title":"Environment Capture","text":"<p>AACO automatically captures: - pip freeze (Python environment) - Environment variables - GPU configuration (rocm-smi) - Clock/governor state - Host information</p>"},{"location":"methodology/#meaningful-comparisons","title":"Meaningful Comparisons","text":"<ol> <li>Same hardware: Compare on identical systems</li> <li>Same workload: Identical models, inputs, config</li> <li>Same conditions: Temperature, power, load</li> <li>Statistical validity: Enough iterations for confidence</li> </ol>"},{"location":"methodology/#profiling-methodology","title":"Profiling Methodology","text":""},{"location":"methodology/#when-to-profile","title":"When to Profile","text":"<ul> <li>After identifying performance target</li> <li>When classifying bottleneck type</li> <li>Investigating specific optimization</li> </ul>"},{"location":"methodology/#rocprof-usage","title":"rocprof Usage","text":"<pre><code># Trace HIP API calls and kernels\naaco run model.onnx --profile\n\n# Full kernel trace\nrocprof --hip-trace --stats ./benchmark\n</code></pre>"},{"location":"methodology/#kernel-analysis","title":"Kernel Analysis","text":"<ol> <li>Identify top kernels: Focus on highest total time</li> <li>Analyze distribution: Look for outliers</li> <li>Check launch patterns: Detect fragmentation</li> <li>Correlate with ONNX nodes: Map to model architecture</li> </ol>"},{"location":"api/analytics/","title":"API Reference: Analytics Module","text":""},{"location":"api/analytics/#aaco.analytics","title":"analytics","text":"<p>AACO Analytics Module - Metrics computation, bottleneck analysis, regression detection.</p>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine","title":"DerivedMetricsEngine","text":"<pre><code>DerivedMetricsEngine()\n</code></pre> <p>Computes derived performance metrics from raw measurements. Combines inference timing, kernel profiling, and telemetry into unified metrics.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def __init__(self):\n    self.inference_results: List[InferenceIteration] = []\n    self.kernel_metrics: Optional[KernelMetrics] = None\n    self.sys_samples: List[SystemSample] = []\n    self.gpu_samples: List[GPUSample] = []\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.add_inference_results","title":"add_inference_results","text":"<pre><code>add_inference_results(results)\n</code></pre> <p>Add inference timing results.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def add_inference_results(self, results: List[InferenceIteration]) -&gt; None:\n    \"\"\"Add inference timing results.\"\"\"\n    self.inference_results.extend(results)\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.set_kernel_metrics","title":"set_kernel_metrics","text":"<pre><code>set_kernel_metrics(metrics)\n</code></pre> <p>Set kernel profiling metrics.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def set_kernel_metrics(self, metrics: KernelMetrics) -&gt; None:\n    \"\"\"Set kernel profiling metrics.\"\"\"\n    self.kernel_metrics = metrics\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.add_system_samples","title":"add_system_samples","text":"<pre><code>add_system_samples(samples)\n</code></pre> <p>Add system telemetry samples.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def add_system_samples(self, samples: List[SystemSample]) -&gt; None:\n    \"\"\"Add system telemetry samples.\"\"\"\n    self.sys_samples.extend(samples)\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.add_gpu_samples","title":"add_gpu_samples","text":"<pre><code>add_gpu_samples(samples)\n</code></pre> <p>Add GPU telemetry samples.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def add_gpu_samples(self, samples: List[GPUSample]) -&gt; None:\n    \"\"\"Add GPU telemetry samples.\"\"\"\n    self.gpu_samples.extend(samples)\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute all derived metrics from collected data.</p> RETURNS DESCRIPTION <code>DerivedMetrics</code> <p>DerivedMetrics containing all computed values.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def compute(self) -&gt; DerivedMetrics:\n    \"\"\"\n    Compute all derived metrics from collected data.\n\n    Returns:\n        DerivedMetrics containing all computed values.\n    \"\"\"\n    # Compute per-phase metrics\n    warmup_phase = self._compute_phase(\"warmup\")\n    measurement_phase = self._compute_phase(\"measurement\")\n\n    # Compute throughput metrics\n    throughput = self._compute_throughput()\n\n    # Compute efficiency metrics\n    efficiency = self._compute_efficiency()\n\n    # Aggregate latency statistics\n    latency = self._compute_latency_stats()\n\n    # System resource utilization\n    system = self._compute_system_utilization()\n\n    # GPU utilization\n    gpu = self._compute_gpu_utilization()\n\n    return DerivedMetrics(\n        warmup_phase=warmup_phase,\n        measurement_phase=measurement_phase,\n        throughput=throughput,\n        efficiency=efficiency,\n        latency=latency,\n        system=system,\n        gpu=gpu,\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.DerivedMetricsEngine.summary_dict","title":"summary_dict","text":"<pre><code>summary_dict()\n</code></pre> <p>Get a dictionary summary of all computed metrics.</p> Source code in <code>aaco/analytics/metrics.py</code> <pre><code>def summary_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Get a dictionary summary of all computed metrics.\"\"\"\n    metrics = self.compute()\n\n    return {\n        \"warmup_iterations\": metrics.warmup_phase.iterations,\n        \"warmup_mean_ms\": metrics.warmup_phase.mean_ms,\n        \"measurement_iterations\": metrics.measurement_phase.iterations,\n        \"measurement_mean_ms\": metrics.measurement_phase.mean_ms,\n        \"measurement_std_ms\": metrics.measurement_phase.std_ms,\n        \"measurement_p99_ms\": metrics.measurement_phase.p99_ms,\n        \"throughput_ips\": metrics.throughput.get(\"inferences_per_sec\", 0),\n        \"gpu_active_ratio\": metrics.efficiency.get(\"gpu_active_ratio\", 0),\n        \"kar\": metrics.efficiency.get(\"kernel_amplification_ratio\", 0),\n        \"microkernel_pct\": metrics.efficiency.get(\"microkernel_pct\", 0),\n        **metrics.gpu,\n        **metrics.system,\n    }\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BottleneckClassifier","title":"BottleneckClassifier","text":"<pre><code>BottleneckClassifier()\n</code></pre> <p>Classifies workload bottlenecks based on collected metrics.</p> <p>Uses a combination of: 1. Rule-based heuristics with thresholds 2. Weighted indicator scoring 3. Multi-dimensional bottleneck fingerprinting</p> Source code in <code>aaco/analytics/classify.py</code> <pre><code>def __init__(self):\n    self.indicators: Dict[str, float] = {}\n    self.scores: Dict[BottleneckCategory, float] = {}\n    self.evidence: List[str] = []\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BottleneckClassifier.classify","title":"classify","text":"<pre><code>classify(metrics=None, kernel_metrics=None, custom_indicators=None)\n</code></pre> <p>Classify the bottleneck based on metrics.</p> PARAMETER DESCRIPTION <code>metrics</code> <p>Derived performance metrics</p> <p> TYPE: <code>Optional[DerivedMetrics]</code> DEFAULT: <code>None</code> </p> <code>kernel_metrics</code> <p>Kernel-level metrics</p> <p> TYPE: <code>Optional[KernelMetrics]</code> DEFAULT: <code>None</code> </p> <code>custom_indicators</code> <p>Additional indicators to consider</p> <p> TYPE: <code>Optional[Dict[str, float]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BottleneckClassification</code> <p>BottleneckClassification with category, confidence, and evidence.</p> Source code in <code>aaco/analytics/classify.py</code> <pre><code>def classify(\n    self,\n    metrics: Optional[DerivedMetrics] = None,\n    kernel_metrics: Optional[KernelMetrics] = None,\n    custom_indicators: Optional[Dict[str, float]] = None,\n) -&gt; BottleneckClassification:\n    \"\"\"\n    Classify the bottleneck based on metrics.\n\n    Args:\n        metrics: Derived performance metrics\n        kernel_metrics: Kernel-level metrics\n        custom_indicators: Additional indicators to consider\n\n    Returns:\n        BottleneckClassification with category, confidence, and evidence.\n    \"\"\"\n    self.indicators = {}\n    self.scores = {cat: 0.0 for cat in BottleneckCategory}\n    self.evidence = []\n\n    # Extract indicators from metrics\n    if metrics:\n        self._extract_from_derived_metrics(metrics)\n\n    if kernel_metrics:\n        self._extract_from_kernel_metrics(kernel_metrics)\n\n    if custom_indicators:\n        self.indicators.update(custom_indicators)\n\n    # Apply classification rules\n    self._apply_compute_rules()\n    self._apply_memory_rules()\n    self._apply_launch_overhead_rules()\n    self._apply_thermal_rules()\n    self._apply_cpu_rules()\n    self._apply_stability_rules()\n\n    # Determine primary bottleneck\n    primary, confidence = self._determine_primary()\n\n    # Build secondary indicators list\n    secondary = [\n        cat.value for cat, score in self.scores.items() if score &gt; 0.3 and cat != primary\n    ]\n\n    return BottleneckClassification(\n        primary=primary.value,\n        secondary=secondary,\n        confidence=confidence,\n        indicators=self.indicators.copy(),\n        evidence=self.evidence.copy(),\n        recommendations=self._generate_recommendations(primary),\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BottleneckClassifier.explain","title":"explain","text":"<pre><code>explain()\n</code></pre> <p>Generate human-readable explanation of classification.</p> Source code in <code>aaco/analytics/classify.py</code> <pre><code>def explain(self) -&gt; str:\n    \"\"\"Generate human-readable explanation of classification.\"\"\"\n    result = self.classify()\n\n    lines = [\n        f\"Primary Bottleneck: {result.primary.upper()}\",\n        f\"Confidence: {result.confidence:.0%}\",\n    ]\n\n    if result.secondary:\n        lines.append(f\"Secondary Factors: {', '.join(result.secondary)}\")\n\n    lines.append(\"\\nEvidence:\")\n    for ev in result.evidence:\n        lines.append(f\"  \u2022 {ev}\")\n\n    lines.append(\"\\nRecommendations:\")\n    for rec in result.recommendations:\n        lines.append(f\"  \u2192 {rec}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.RegressionDetector","title":"RegressionDetector","text":"<pre><code>RegressionDetector(thresholds=None)\n</code></pre> <p>Detects performance regressions between benchmark sessions.</p> <p>Supports: 1. A/B comparison (baseline vs current) 2. Trend analysis over multiple sessions 3. Statistical significance testing</p> Source code in <code>aaco/analytics/diff.py</code> <pre><code>def __init__(self, thresholds: Optional[RegressionThresholds] = None):\n    self.thresholds = thresholds or RegressionThresholds()\n    self.comparisons: List[MetricComparison] = []\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.RegressionDetector.compare_metrics","title":"compare_metrics","text":"<pre><code>compare_metrics(baseline, current, baseline_raw=None, current_raw=None)\n</code></pre> <p>Compare two sets of metrics and produce a regression verdict.</p> PARAMETER DESCRIPTION <code>baseline</code> <p>Metrics from baseline (reference) run</p> <p> TYPE: <code>DerivedMetrics</code> </p> <code>current</code> <p>Metrics from current (new) run</p> <p> TYPE: <code>DerivedMetrics</code> </p> <code>baseline_raw</code> <p>Raw latency values from baseline (for stats)</p> <p> TYPE: <code>Optional[List[float]]</code> DEFAULT: <code>None</code> </p> <code>current_raw</code> <p>Raw latency values from current (for stats)</p> <p> TYPE: <code>Optional[List[float]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RegressionVerdict</code> <p>RegressionVerdict with overall assessment.</p> Source code in <code>aaco/analytics/diff.py</code> <pre><code>def compare_metrics(\n    self,\n    baseline: DerivedMetrics,\n    current: DerivedMetrics,\n    baseline_raw: Optional[List[float]] = None,\n    current_raw: Optional[List[float]] = None,\n) -&gt; RegressionVerdict:\n    \"\"\"\n    Compare two sets of metrics and produce a regression verdict.\n\n    Args:\n        baseline: Metrics from baseline (reference) run\n        current: Metrics from current (new) run\n        baseline_raw: Raw latency values from baseline (for stats)\n        current_raw: Raw latency values from current (for stats)\n\n    Returns:\n        RegressionVerdict with overall assessment.\n    \"\"\"\n    self.comparisons = []\n\n    # Compare phase metrics\n    self._compare_phase(\n        baseline.measurement_phase,\n        current.measurement_phase,\n        \"measurement\",\n    )\n\n    # Compare key metrics\n    key_comparisons = [\n        (\n            \"throughput_ips\",\n            baseline.throughput.get(\"inferences_per_sec\", 0),\n            current.throughput.get(\"inferences_per_sec\", 0),\n            True,\n        ),  # higher is better\n        (\n            \"gpu_active_ratio\",\n            baseline.efficiency.get(\"gpu_active_ratio\", 0),\n            current.efficiency.get(\"gpu_active_ratio\", 0),\n            True,\n        ),\n        (\n            \"microkernel_pct\",\n            baseline.efficiency.get(\"microkernel_pct\", 0),\n            current.efficiency.get(\"microkernel_pct\", 0),\n            False,\n        ),  # lower is better\n        (\n            \"gpu_util_pct\",\n            baseline.gpu.get(\"gpu_util_mean_pct\", 0),\n            current.gpu.get(\"gpu_util_mean_pct\", 0),\n            True,\n        ),\n        (\n            \"power_w\",\n            baseline.gpu.get(\"power_mean_w\", 0),\n            current.gpu.get(\"power_mean_w\", 0),\n            False,\n        ),  # lower is better (efficiency)\n    ]\n\n    for name, base_val, curr_val, higher_better in key_comparisons:\n        self._compare_metric(name, base_val, curr_val, higher_better)\n\n    # Statistical test if raw data available\n    p_value = None\n    if baseline_raw and current_raw:\n        p_value = self._statistical_test(baseline_raw, current_raw)\n\n    # Determine overall verdict\n    verdict, confidence = self._determine_verdict(p_value)\n\n    # Build detailed summary\n    regressions = [c for c in self.comparisons if c.verdict == \"regression\"]\n    improvements = [c for c in self.comparisons if c.verdict == \"improvement\"]\n\n    return RegressionVerdict(\n        verdict=verdict,\n        confidence=confidence,\n        regressions=[c.metric_name for c in regressions],\n        improvements=[c.metric_name for c in improvements],\n        comparisons=[\n            {\n                \"metric\": c.metric_name,\n                \"baseline\": c.baseline_value,\n                \"current\": c.current_value,\n                \"delta_pct\": c.delta_pct,\n                \"verdict\": c.verdict,\n            }\n            for c in self.comparisons\n        ],\n        p_value=p_value,\n        summary=self._build_summary(verdict, regressions, improvements),\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.RegressionDetector.compare_sessions","title":"compare_sessions","text":"<pre><code>compare_sessions(baseline_path, current_path)\n</code></pre> <p>Compare two session bundles.</p> PARAMETER DESCRIPTION <code>baseline_path</code> <p>Path to baseline session folder</p> <p> TYPE: <code>Path</code> </p> <code>current_path</code> <p>Path to current session folder</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>RegressionVerdict</code> <p>RegressionVerdict</p> Source code in <code>aaco/analytics/diff.py</code> <pre><code>def compare_sessions(\n    self,\n    baseline_path: Path,\n    current_path: Path,\n) -&gt; RegressionVerdict:\n    \"\"\"\n    Compare two session bundles.\n\n    Args:\n        baseline_path: Path to baseline session folder\n        current_path: Path to current session folder\n\n    Returns:\n        RegressionVerdict\n    \"\"\"\n\n    # Load session metadata\n    def load_metrics(\n        session_path: Path,\n    ) -&gt; Tuple[Optional[DerivedMetrics], List[float]]:\n        metrics_file = session_path / \"metrics.json\"\n        inference_file = session_path / \"inference_results.json\"\n\n        derived = None\n        raw_latencies = []\n\n        if metrics_file.exists():\n            with open(metrics_file) as f:\n                # Reconstruct DerivedMetrics from JSON\n                pass  # Would need proper deserialization\n\n        if inference_file.exists():\n            with open(inference_file) as f:\n                data = json.load(f)\n                raw_latencies = [\n                    r[\"latency_ms\"] for r in data if r.get(\"phase\") == \"measurement\"\n                ]\n\n        return derived, raw_latencies\n\n    base_metrics, base_raw = load_metrics(baseline_path)\n    curr_metrics, curr_raw = load_metrics(current_path)\n\n    if base_metrics and curr_metrics:\n        return self.compare_metrics(base_metrics, curr_metrics, base_raw, curr_raw)\n\n    # Fall back to raw comparison\n    if base_raw and curr_raw:\n        return self._compare_raw_latencies(base_raw, curr_raw)\n\n    return RegressionVerdict(\n        verdict=\"UNKNOWN\",\n        confidence=0,\n        regressions=[],\n        improvements=[],\n        comparisons=[],\n        summary=\"Insufficient data for comparison\",\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchScalingAnalyzer","title":"BatchScalingAnalyzer","text":"<pre><code>BatchScalingAnalyzer()\n</code></pre> <p>Analyzes how model performance scales with batch size.</p> <p>Key insights: - Throughput scaling efficiency (how close to linear) - Saturation point (where throughput plateaus) - Memory pressure thresholds - Optimal batch size for throughput/latency tradeoff</p> Source code in <code>aaco/analytics/batch_scaler.py</code> <pre><code>def __init__(self):\n    self.points: List[BatchPoint] = []\n    self.vram_limit_mb: Optional[float] = None\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchScalingAnalyzer.add_measurement","title":"add_measurement","text":"<pre><code>add_measurement(batch_size, latencies_ms, vram_mb=None, gpu_util=None, power_w=None)\n</code></pre> <p>Add a measurement at a specific batch size.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>Batch size used</p> <p> TYPE: <code>int</code> </p> <code>latencies_ms</code> <p>List of per-inference latencies</p> <p> TYPE: <code>List[float]</code> </p> <code>vram_mb</code> <p>VRAM usage at this batch</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>gpu_util</code> <p>GPU utilization %</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>power_w</code> <p>Power consumption</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BatchPoint</code> <p>BatchPoint with computed metrics.</p> Source code in <code>aaco/analytics/batch_scaler.py</code> <pre><code>def add_measurement(\n    self,\n    batch_size: int,\n    latencies_ms: List[float],\n    vram_mb: Optional[float] = None,\n    gpu_util: Optional[float] = None,\n    power_w: Optional[float] = None,\n) -&gt; BatchPoint:\n    \"\"\"\n    Add a measurement at a specific batch size.\n\n    Args:\n        batch_size: Batch size used\n        latencies_ms: List of per-inference latencies\n        vram_mb: VRAM usage at this batch\n        gpu_util: GPU utilization %\n        power_w: Power consumption\n\n    Returns:\n        BatchPoint with computed metrics.\n    \"\"\"\n    arr = np.array(latencies_ms)\n\n    mean_ms = float(np.mean(arr))\n    throughput_batches = 1000.0 / mean_ms  # batches/sec\n    throughput_samples = throughput_batches * batch_size  # samples/sec\n\n    point = BatchPoint(\n        batch_size=batch_size,\n        latency_p50_ms=float(np.percentile(arr, 50)),\n        latency_p90_ms=float(np.percentile(arr, 90)),\n        latency_p99_ms=float(np.percentile(arr, 99)),\n        latency_mean_ms=mean_ms,\n        latency_std_ms=float(np.std(arr)),\n        throughput_samples_per_sec=throughput_samples,\n        throughput_batches_per_sec=throughput_batches,\n        vram_usage_mb=vram_mb,\n        gpu_util_pct=gpu_util,\n        power_w=power_w,\n    )\n\n    # Insert in sorted order\n    self.points.append(point)\n    self.points.sort(key=lambda p: p.batch_size)\n\n    return point\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchScalingAnalyzer.analyze","title":"analyze","text":"<pre><code>analyze()\n</code></pre> <p>Perform complete scaling analysis.</p> RETURNS DESCRIPTION <code>ScalingAnalysis</code> <p>ScalingAnalysis with all insights.</p> Source code in <code>aaco/analytics/batch_scaler.py</code> <pre><code>def analyze(self) -&gt; ScalingAnalysis:\n    \"\"\"\n    Perform complete scaling analysis.\n\n    Returns:\n        ScalingAnalysis with all insights.\n    \"\"\"\n    if len(self.points) &lt; 2:\n        return self._minimal_analysis()\n\n    batch_sizes = [p.batch_size for p in self.points]\n    throughputs = [p.throughput_samples_per_sec for p in self.points]\n    latencies = [p.latency_mean_ms for p in self.points]\n\n    # Compute scaling efficiency\n    efficiency = self._compute_scaling_efficiency(batch_sizes, throughputs)\n\n    # Find saturation point\n    saturation = self._find_saturation_point(batch_sizes, throughputs)\n\n    # Find memory bound point\n    memory_bound = self._find_memory_bound_point()\n\n    # Find optimal batch\n    optimal = self._find_optimal_batch()\n\n    # Fit curves\n    throughput_fit = self._fit_curve(batch_sizes, throughputs)\n    latency_fit = self._fit_curve(batch_sizes, latencies)\n\n    # Detect bottleneck transitions\n    transition = self._detect_bottleneck_transition()\n\n    # Generate analysis\n    summary = self._generate_summary(efficiency, saturation, memory_bound, optimal)\n    recommendations = self._generate_recommendations(efficiency, saturation, memory_bound)\n\n    return ScalingAnalysis(\n        batch_sizes=batch_sizes,\n        points=self.points,\n        scaling_efficiency=efficiency,\n        saturation_batch=saturation,\n        memory_bound_batch=memory_bound,\n        optimal_batch=optimal,\n        throughput_curve_fit=throughput_fit,\n        latency_curve_fit=latency_fit,\n        bottleneck_transition=transition,\n        analysis_summary=summary,\n        recommendations=recommendations,\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchScalingAnalyzer.get_scaling_table","title":"get_scaling_table","text":"<pre><code>get_scaling_table()\n</code></pre> <p>Get tabular view of scaling data.</p> Source code in <code>aaco/analytics/batch_scaler.py</code> <pre><code>def get_scaling_table(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get tabular view of scaling data.\"\"\"\n    return [\n        {\n            \"batch\": p.batch_size,\n            \"latency_ms\": f\"{p.latency_mean_ms:.2f}\",\n            \"throughput\": f\"{p.throughput_samples_per_sec:.1f}\",\n            \"vram_mb\": f\"{p.vram_usage_mb:.0f}\" if p.vram_usage_mb else \"-\",\n            \"gpu_util\": f\"{p.gpu_util_pct:.1f}%\" if p.gpu_util_pct else \"-\",\n        }\n        for p in self.points\n    ]\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchScalingAnalyzer.plot_scaling_curves","title":"plot_scaling_curves","text":"<pre><code>plot_scaling_curves(output_path=None)\n</code></pre> <p>Generate scaling plots.</p> <p>Plots: 1. Throughput vs Batch Size 2. Latency vs Batch Size 3. Efficiency curve</p> Source code in <code>aaco/analytics/batch_scaler.py</code> <pre><code>def plot_scaling_curves(self, output_path: Optional[str] = None):\n    \"\"\"\n    Generate scaling plots.\n\n    Plots:\n    1. Throughput vs Batch Size\n    2. Latency vs Batch Size\n    3. Efficiency curve\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        logger.warning(\"matplotlib not installed, skipping plots\")\n        return None\n\n    if len(self.points) &lt; 2:\n        return None\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    fig.suptitle(\"Batch Scaling Analysis\", fontsize=14, fontweight=\"bold\")\n\n    batches = [p.batch_size for p in self.points]\n    throughputs = [p.throughput_samples_per_sec for p in self.points]\n    latencies = [p.latency_mean_ms for p in self.points]\n\n    # Plot 1: Throughput\n    ax1 = axes[0, 0]\n    ax1.plot(batches, throughputs, \"b-o\", linewidth=2, markersize=8)\n    ax1.set_xlabel(\"Batch Size\")\n    ax1.set_ylabel(\"Throughput (samples/sec)\")\n    ax1.set_title(\"Throughput Scaling\")\n    ax1.grid(True, alpha=0.3)\n\n    # Add ideal linear line\n    ideal = [throughputs[0] * b / batches[0] for b in batches]\n    ax1.plot(batches, ideal, \"g--\", alpha=0.5, label=\"Ideal Linear\")\n    ax1.legend()\n\n    # Plot 2: Latency\n    ax2 = axes[0, 1]\n    ax2.plot(batches, latencies, \"r-o\", linewidth=2, markersize=8)\n    ax2.set_xlabel(\"Batch Size\")\n    ax2.set_ylabel(\"Latency (ms)\")\n    ax2.set_title(\"Latency vs Batch Size\")\n    ax2.grid(True, alpha=0.3)\n\n    # Plot 3: Efficiency\n    ax3 = axes[1, 0]\n    efficiencies = [\n        throughputs[i] / (throughputs[0] * batches[i] / batches[0]) for i in range(len(batches))\n    ]\n    ax3.bar(range(len(batches)), efficiencies, color=\"purple\", alpha=0.7)\n    ax3.set_xticks(range(len(batches)))\n    ax3.set_xticklabels([str(b) for b in batches])\n    ax3.set_xlabel(\"Batch Size\")\n    ax3.set_ylabel(\"Scaling Efficiency\")\n    ax3.set_title(\"Scaling Efficiency (1.0 = Perfect)\")\n    ax3.axhline(y=1.0, color=\"g\", linestyle=\"--\", alpha=0.5)\n    ax3.set_ylim(0, 1.2)\n\n    # Plot 4: VRAM if available\n    ax4 = axes[1, 1]\n    vrams = [p.vram_usage_mb for p in self.points if p.vram_usage_mb]\n    if vrams and len(vrams) == len(batches):\n        ax4.plot(batches, vrams, \"orange\", marker=\"s\", linewidth=2, markersize=8)\n        ax4.set_xlabel(\"Batch Size\")\n        ax4.set_ylabel(\"VRAM Usage (MB)\")\n        ax4.set_title(\"Memory Scaling\")\n        ax4.grid(True, alpha=0.3)\n    else:\n        ax4.text(\n            0.5,\n            0.5,\n            \"VRAM data not available\",\n            ha=\"center\",\n            va=\"center\",\n            transform=ax4.transAxes,\n        )\n        ax4.set_title(\"Memory Scaling\")\n\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        logger.info(f\"Saved scaling plots to {output_path}\")\n\n    return fig\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.BatchPoint","title":"BatchPoint  <code>dataclass</code>","text":"<pre><code>BatchPoint(batch_size, latency_p50_ms, latency_p90_ms, latency_p99_ms, latency_mean_ms, latency_std_ms, throughput_samples_per_sec, throughput_batches_per_sec, vram_usage_mb=None, gpu_util_pct=None, power_w=None)\n</code></pre> <p>Single measurement point at a specific batch size.</p>"},{"location":"api/analytics/#aaco.analytics.ScalingAnalysis","title":"ScalingAnalysis  <code>dataclass</code>","text":"<pre><code>ScalingAnalysis(batch_sizes, points, scaling_efficiency, saturation_batch, memory_bound_batch, optimal_batch, throughput_curve_fit, latency_curve_fit, bottleneck_transition, analysis_summary, recommendations)\n</code></pre> <p>Complete batch scaling analysis results.</p>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator","title":"TimelineCorrelator","text":"<pre><code>TimelineCorrelator(t0_ns=0)\n</code></pre> <p>Correlates events from all observability planes to unified timeline.</p> <p>Key capabilities: - Align all events to common t0 - Window-based correlation for inference iterations - Spike detection and attribution - Cross-plane causality inference</p> PARAMETER DESCRIPTION <code>t0_ns</code> <p>Session start time in nanoseconds (monotonic)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def __init__(self, t0_ns: int = 0):\n    \"\"\"\n    Args:\n        t0_ns: Session start time in nanoseconds (monotonic)\n    \"\"\"\n    self.t0_ns = t0_ns\n    self.events: List[TimelineEvent] = []\n    self._sorted = False\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.set_t0","title":"set_t0","text":"<pre><code>set_t0(t0_ns)\n</code></pre> <p>Set the reference time for all events.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def set_t0(self, t0_ns: int) -&gt; None:\n    \"\"\"Set the reference time for all events.\"\"\"\n    self.t0_ns = t0_ns\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.add_event","title":"add_event","text":"<pre><code>add_event(t_ns, source, event_type, payload=None, duration_ns=None, correlation_id=None)\n</code></pre> <p>Add an event to the timeline.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def add_event(\n    self,\n    t_ns: int,\n    source: EventSource,\n    event_type: EventType,\n    payload: Optional[Dict[str, Any]] = None,\n    duration_ns: Optional[int] = None,\n    correlation_id: Optional[str] = None,\n) -&gt; TimelineEvent:\n    \"\"\"Add an event to the timeline.\"\"\"\n    # Normalize to relative time\n    relative_t = t_ns - self.t0_ns if self.t0_ns &gt; 0 else t_ns\n\n    event = TimelineEvent(\n        t_ns=relative_t,\n        source=source,\n        event_type=event_type,\n        payload=payload or {},\n        duration_ns=duration_ns,\n        correlation_id=correlation_id,\n    )\n    self.events.append(event)\n    self._sorted = False\n    return event\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.add_inference_iteration","title":"add_inference_iteration","text":"<pre><code>add_inference_iteration(iter_idx, t_start_ns, t_end_ns, latency_ms)\n</code></pre> <p>Add inference iteration boundary events.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def add_inference_iteration(\n    self, iter_idx: int, t_start_ns: int, t_end_ns: int, latency_ms: float\n) -&gt; None:\n    \"\"\"Add inference iteration boundary events.\"\"\"\n    corr_id = f\"iter_{iter_idx}\"\n\n    self.add_event(\n        t_ns=t_start_ns,\n        source=EventSource.INFERENCE,\n        event_type=EventType.INFERENCE_START,\n        payload={\"iter_idx\": iter_idx},\n        correlation_id=corr_id,\n    )\n\n    self.add_event(\n        t_ns=t_end_ns,\n        source=EventSource.INFERENCE,\n        event_type=EventType.INFERENCE_END,\n        payload={\"iter_idx\": iter_idx, \"latency_ms\": latency_ms},\n        duration_ns=t_end_ns - t_start_ns,\n        correlation_id=corr_id,\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.add_kernel_execution","title":"add_kernel_execution","text":"<pre><code>add_kernel_execution(kernel_name, t_start_ns, t_end_ns)\n</code></pre> <p>Add GPU kernel execution event.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def add_kernel_execution(self, kernel_name: str, t_start_ns: int, t_end_ns: int) -&gt; None:\n    \"\"\"Add GPU kernel execution event.\"\"\"\n    self.add_event(\n        t_ns=t_start_ns,\n        source=EventSource.KERNEL,\n        event_type=EventType.KERNEL_LAUNCH,\n        payload={\n            \"kernel_name\": kernel_name,\n        },\n        duration_ns=t_end_ns - t_start_ns,\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.add_system_sample","title":"add_system_sample","text":"<pre><code>add_system_sample(t_ns, cpu_pct, rss_mb, ctx_switches)\n</code></pre> <p>Add system telemetry sample.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def add_system_sample(\n    self, t_ns: int, cpu_pct: float, rss_mb: float, ctx_switches: int\n) -&gt; None:\n    \"\"\"Add system telemetry sample.\"\"\"\n    self.add_event(\n        t_ns=t_ns,\n        source=EventSource.SYSTEM,\n        event_type=EventType.CPU_SAMPLE,\n        payload={\n            \"cpu_pct\": cpu_pct,\n            \"rss_mb\": rss_mb,\n            \"ctx_switches\": ctx_switches,\n        },\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.add_gpu_sample","title":"add_gpu_sample","text":"<pre><code>add_gpu_sample(t_ns, gfx_clock, power_w, temp_c, util_pct)\n</code></pre> <p>Add GPU telemetry sample.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def add_gpu_sample(\n    self,\n    t_ns: int,\n    gfx_clock: float,\n    power_w: float,\n    temp_c: float,\n    util_pct: float,\n) -&gt; None:\n    \"\"\"Add GPU telemetry sample.\"\"\"\n    self.add_event(\n        t_ns=t_ns,\n        source=EventSource.GPU,\n        event_type=EventType.GPU_SAMPLE,\n        payload={\n            \"gfx_clock_mhz\": gfx_clock,\n            \"power_w\": power_w,\n            \"temp_c\": temp_c,\n            \"gpu_util_pct\": util_pct,\n        },\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.get_events_in_window","title":"get_events_in_window","text":"<pre><code>get_events_in_window(start_ns, end_ns, source=None)\n</code></pre> <p>Get all events within a time window.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def get_events_in_window(\n    self, start_ns: int, end_ns: int, source: Optional[EventSource] = None\n) -&gt; List[TimelineEvent]:\n    \"\"\"Get all events within a time window.\"\"\"\n    self._ensure_sorted()\n\n    result = []\n    for event in self.events:\n        if event.t_ns &lt; start_ns:\n            continue\n        if event.t_ns &gt; end_ns:\n            break\n        if source is None or event.source == source:\n            result.append(event)\n\n    return result\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.correlate_inference_iterations","title":"correlate_inference_iterations","text":"<pre><code>correlate_inference_iterations()\n</code></pre> <p>Create correlation windows for each inference iteration. Each window contains all events during that iteration.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def correlate_inference_iterations(self) -&gt; List[CorrelationWindow]:\n    \"\"\"\n    Create correlation windows for each inference iteration.\n    Each window contains all events during that iteration.\n    \"\"\"\n    self._ensure_sorted()\n\n    # Find inference boundaries\n    start_events = [e for e in self.events if e.event_type == EventType.INFERENCE_START]\n    end_events = {\n        e.correlation_id: e for e in self.events if e.event_type == EventType.INFERENCE_END\n    }\n\n    windows = []\n    for start in start_events:\n        end = end_events.get(start.correlation_id)\n        if not end:\n            continue\n\n        window_events = self.get_events_in_window(start.t_ns, end.t_ns)\n\n        window = CorrelationWindow(\n            start_ns=start.t_ns,\n            end_ns=end.t_ns,\n            inference_events=[e for e in window_events if e.source == EventSource.INFERENCE],\n            kernel_events=[e for e in window_events if e.source == EventSource.KERNEL],\n            system_events=[e for e in window_events if e.source == EventSource.SYSTEM],\n            gpu_events=[e for e in window_events if e.source == EventSource.GPU],\n        )\n        windows.append(window)\n\n    return windows\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.detect_latency_spikes","title":"detect_latency_spikes","text":"<pre><code>detect_latency_spikes(threshold_pct=50.0)\n</code></pre> <p>Detect inference iterations with abnormal latency.</p> PARAMETER DESCRIPTION <code>threshold_pct</code> <p>% above median to flag as spike</p> <p> TYPE: <code>float</code> DEFAULT: <code>50.0</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>List of spike events with correlation evidence.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def detect_latency_spikes(self, threshold_pct: float = 50.0) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Detect inference iterations with abnormal latency.\n\n    Args:\n        threshold_pct: % above median to flag as spike\n\n    Returns:\n        List of spike events with correlation evidence.\n    \"\"\"\n    windows = self.correlate_inference_iterations()\n    if len(windows) &lt; 5:\n        return []\n\n    # Compute median latency\n    latencies = [w.duration_ms for w in windows]\n    median = np.median(latencies)\n    threshold = median * (1 + threshold_pct / 100)\n\n    spikes = []\n    for i, window in enumerate(windows):\n        if window.duration_ms &gt; threshold:\n            # Analyze what happened during spike\n            evidence = self._analyze_spike(window, windows, median)\n\n            spikes.append(\n                {\n                    \"iteration\": i,\n                    \"latency_ms\": window.duration_ms,\n                    \"median_ms\": median,\n                    \"spike_pct\": ((window.duration_ms - median) / median * 100),\n                    \"evidence\": evidence,\n                    \"t_start_ns\": window.start_ns,\n                }\n            )\n\n    return spikes\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.compute_gpu_active_timeline","title":"compute_gpu_active_timeline","text":"<pre><code>compute_gpu_active_timeline(resolution_ms=10.0)\n</code></pre> <p>Compute GPU active ratio over time at given resolution.</p> RETURNS DESCRIPTION <code>Dict[str, List[float]]</code> <p>Dict with 't_ms' and 'gpu_active_pct' time series.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def compute_gpu_active_timeline(self, resolution_ms: float = 10.0) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Compute GPU active ratio over time at given resolution.\n\n    Returns:\n        Dict with 't_ms' and 'gpu_active_pct' time series.\n    \"\"\"\n    self._ensure_sorted()\n\n    if not self.events:\n        return {\"t_ms\": [], \"gpu_active_pct\": []}\n\n    max_t = max(e.t_ns for e in self.events)\n    resolution_ns = int(resolution_ms * 1e6)\n\n    t_points = []\n    active_pcts = []\n\n    t = 0\n    while t &lt; max_t:\n        window_end = t + resolution_ns\n\n        # Get kernel events in this window\n        kernel_events = self.get_events_in_window(t, window_end, EventSource.KERNEL)\n\n        # Sum kernel durations\n        kernel_time = sum(e.duration_ns or 0 for e in kernel_events)\n        active_pct = (kernel_time / resolution_ns * 100) if resolution_ns &gt; 0 else 0\n\n        t_points.append(t / 1e6)  # Convert to ms\n        active_pcts.append(min(100, active_pct))\n\n        t += resolution_ns\n\n    return {\"t_ms\": t_points, \"gpu_active_pct\": active_pcts}\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.generate_insights","title":"generate_insights","text":"<pre><code>generate_insights()\n</code></pre> <p>Generate insights from correlation analysis.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def generate_insights(self) -&gt; List[CorrelationInsight]:\n    \"\"\"Generate insights from correlation analysis.\"\"\"\n    insights = []\n\n    # Check for latency spikes\n    spikes = self.detect_latency_spikes()\n    if spikes:\n        insights.append(\n            CorrelationInsight(\n                category=\"latency_spike\",\n                severity=\"warning\" if len(spikes) &lt; 3 else \"critical\",\n                description=f\"Detected {len(spikes)} latency spike(s)\",\n                evidence={\"spike_count\": len(spikes), \"spikes\": spikes[:5]},\n            )\n        )\n\n    # Compute overall GPU activity\n    windows = self.correlate_inference_iterations()\n    if windows:\n        avg_gpu_active = np.mean([w.get_summary()[\"gpu_active_ratio\"] for w in windows])\n\n        if avg_gpu_active &lt; 0.5:\n            insights.append(\n                CorrelationInsight(\n                    category=\"low_gpu_utilization\",\n                    severity=\"warning\",\n                    description=f\"GPU active only {avg_gpu_active:.0%} during inference\",\n                    evidence={\"gpu_active_ratio\": avg_gpu_active},\n                )\n            )\n\n    # Check for system noise correlation\n    system_events = [e for e in self.events if e.source == EventSource.SYSTEM]\n    if system_events:\n        ctx_switches = [e.payload.get(\"ctx_switches\", 0) for e in system_events]\n        if np.std(ctx_switches) &gt; np.mean(ctx_switches) * 0.5:\n            insights.append(\n                CorrelationInsight(\n                    category=\"system_noise\",\n                    severity=\"info\",\n                    description=\"High variance in context switches detected\",\n                    evidence={\n                        \"ctx_switch_mean\": float(np.mean(ctx_switches)),\n                        \"ctx_switch_std\": float(np.std(ctx_switches)),\n                    },\n                )\n            )\n\n    return insights\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.export_timeline","title":"export_timeline","text":"<pre><code>export_timeline()\n</code></pre> <p>Export full timeline as list of dicts.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def export_timeline(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Export full timeline as list of dicts.\"\"\"\n    self._ensure_sorted()\n    return [e.to_dict() for e in self.events]\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineCorrelator.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get timeline summary statistics.</p> Source code in <code>aaco/analytics/timeline.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get timeline summary statistics.\"\"\"\n    self._ensure_sorted()\n\n    if not self.events:\n        return {\"total_events\": 0}\n\n    by_source = {}\n    for source in EventSource:\n        count = sum(1 for e in self.events if e.source == source)\n        if count &gt; 0:\n            by_source[source.value] = count\n\n    duration_ns = self.events[-1].t_ns - self.events[0].t_ns\n\n    return {\n        \"total_events\": len(self.events),\n        \"by_source\": by_source,\n        \"duration_ms\": duration_ns / 1e6,\n        \"time_range\": {\n            \"start_ns\": self.events[0].t_ns,\n            \"end_ns\": self.events[-1].t_ns,\n        },\n    }\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.TimelineEvent","title":"TimelineEvent  <code>dataclass</code>","text":"<pre><code>TimelineEvent(t_ns, source, event_type, payload=dict(), duration_ns=None, correlation_id=None)\n</code></pre> <p>Single event on the unified timeline.</p>"},{"location":"api/analytics/#aaco.analytics.LaunchTaxAnalyzer","title":"LaunchTaxAnalyzer","text":"<pre><code>LaunchTaxAnalyzer(kernel_summaries, wall_clock_ms, num_inferences=1, microkernel_threshold_us=DEFAULT_MICROKERNEL_THRESHOLD_US)\n</code></pre> <p>Analyzes GPU kernel launch overhead and microkernel patterns.</p> <p>Key metrics: - Microkernel percentage (kernels under threshold) - Launch rate (kernels per second) - Launch tax score (composite overhead indicator) - Duration distribution analysis</p> <p>This is the signature feature for GPU performance analysis: - Identifies \"too many tiny kernels\" anti-pattern - Quantifies launch overhead impact - Provides fusion recommendations</p> PARAMETER DESCRIPTION <code>kernel_summaries</code> <p>List of kernel summary stats</p> <p> TYPE: <code>List[KernelSummary]</code> </p> <code>wall_clock_ms</code> <p>Total wall clock time</p> <p> TYPE: <code>float</code> </p> <code>num_inferences</code> <p>Number of inference iterations</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>microkernel_threshold_us</code> <p>Threshold for \"tiny\" kernels</p> <p> TYPE: <code>float</code> DEFAULT: <code>DEFAULT_MICROKERNEL_THRESHOLD_US</code> </p> Source code in <code>aaco/analytics/launch_tax.py</code> <pre><code>def __init__(\n    self,\n    kernel_summaries: List[KernelSummary],\n    wall_clock_ms: float,\n    num_inferences: int = 1,\n    microkernel_threshold_us: float = DEFAULT_MICROKERNEL_THRESHOLD_US,\n):\n    \"\"\"\n    Args:\n        kernel_summaries: List of kernel summary stats\n        wall_clock_ms: Total wall clock time\n        num_inferences: Number of inference iterations\n        microkernel_threshold_us: Threshold for \"tiny\" kernels\n    \"\"\"\n    self.kernels = kernel_summaries\n    self.wall_clock_ms = wall_clock_ms\n    self.num_inferences = num_inferences\n    self.threshold_us = microkernel_threshold_us\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.LaunchTaxAnalyzer.analyze","title":"analyze","text":"<pre><code>analyze()\n</code></pre> <p>Perform complete launch tax analysis.</p> Source code in <code>aaco/analytics/launch_tax.py</code> <pre><code>def analyze(self) -&gt; LaunchTaxReport:\n    \"\"\"Perform complete launch tax analysis.\"\"\"\n    if not self.kernels:\n        return self._empty_report()\n\n    # Compute core metrics\n    metrics = self._compute_metrics()\n\n    # Build duration histogram\n    histogram = self._build_duration_histogram()\n\n    # Find top microkernels\n    top_micro = self._find_top_microkernels()\n\n    # Find top heavy kernels\n    top_heavy = self._find_top_heavy_kernels()\n\n    # Extract kernel name patterns\n    patterns = self._extract_kernel_patterns()\n\n    # Determine assessment\n    assessment = self._assess_severity(metrics)\n\n    # Generate recommendations\n    recommendations = self._generate_recommendations(metrics, top_micro)\n\n    # Generate detailed diagnosis\n    diagnosis = self._generate_diagnosis(metrics, histogram, top_micro)\n\n    return LaunchTaxReport(\n        metrics=metrics,\n        duration_histogram=histogram,\n        top_microkernels=top_micro,\n        top_heavy_kernels=top_heavy,\n        kernel_name_patterns=patterns,\n        assessment=assessment,\n        recommendations=recommendations,\n        detailed_diagnosis=diagnosis,\n    )\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.LaunchTaxAnalyzer.plot_duration_histogram","title":"plot_duration_histogram","text":"<pre><code>plot_duration_histogram(output_path=None)\n</code></pre> <p>Generate kernel duration histogram plot.</p> Source code in <code>aaco/analytics/launch_tax.py</code> <pre><code>def plot_duration_histogram(self, output_path: Optional[str] = None):\n    \"\"\"Generate kernel duration histogram plot.\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        logger.warning(\"matplotlib not available for plotting\")\n        return None\n\n    histogram = self._build_duration_histogram()\n\n    # Filter non-empty buckets\n    data = [\n        (b.range_us, b.count, b.pct_of_time, b.is_microkernel) for b in histogram if b.count &gt; 0\n    ]\n\n    if not data:\n        return None\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle(\"Kernel Duration Analysis\", fontsize=14, fontweight=\"bold\")\n\n    # Left: Call count by duration\n    labels = [f\"{d[0][0]}-{d[0][1] if d[0][1] &lt; 999999 else '\u221e'}\u03bcs\" for d in data]\n    counts = [d[1] for d in data]\n    colors = [\"red\" if d[3] else \"steelblue\" for d in data]\n\n    ax1.barh(labels, counts, color=colors, alpha=0.7)\n    ax1.set_xlabel(\"Kernel Call Count\")\n    ax1.set_title(\"Calls by Duration Bucket\")\n    ax1.axvline(x=0, color=\"gray\", linewidth=0.5)\n\n    # Add microkernel annotation\n    ax1.text(\n        0.95,\n        0.05,\n        \"Red = Microkernel (&lt;10\u03bcs)\",\n        transform=ax1.transAxes,\n        ha=\"right\",\n        fontsize=9,\n        color=\"red\",\n    )\n\n    # Right: Time contribution\n    time_pcts = [d[2] for d in data]\n\n    ax2.barh(labels, time_pcts, color=colors, alpha=0.7)\n    ax2.set_xlabel(\"% of Total GPU Time\")\n    ax2.set_title(\"Time Contribution by Duration\")\n\n    plt.tight_layout()\n\n    if output_path:\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        logger.info(f\"Saved launch tax plot to {output_path}\")\n\n    return fig\n</code></pre>"},{"location":"api/analytics/#aaco.analytics.LaunchTaxReport","title":"LaunchTaxReport  <code>dataclass</code>","text":"<pre><code>LaunchTaxReport(metrics, duration_histogram, top_microkernels, top_heavy_kernels, kernel_name_patterns, assessment, recommendations, detailed_diagnosis)\n</code></pre> <p>Complete launch tax analysis report.</p>"},{"location":"api/analytics/#overview","title":"Overview","text":"<p>The analytics module provides statistical analysis, bottleneck classification, and root cause inference.</p>"},{"location":"api/analytics/#statistics","title":"Statistics","text":""},{"location":"api/analytics/#statisticalsummary","title":"StatisticalSummary","text":"<pre><code>from aaco.analytics import StatisticalSummary\n\nsummary = StatisticalSummary(data)\n\nprint(summary.mean)\nprint(summary.median)\nprint(summary.std)\nprint(summary.percentile(95))\nprint(summary.cv)  # Coefficient of variation\n</code></pre>"},{"location":"api/analytics/#outlier-detection","title":"Outlier Detection","text":"<pre><code>from aaco.analytics import OutlierDetector\n\ndetector = OutlierDetector(method=\"iqr\", threshold=1.5)\nclean_data = detector.filter(data)\noutliers = detector.detect(data)\n</code></pre> <p>Methods: - <code>iqr</code>: Interquartile range (default) - <code>zscore</code>: Z-score based - <code>mad</code>: Median absolute deviation</p>"},{"location":"api/analytics/#bottleneck-classification","title":"Bottleneck Classification","text":""},{"location":"api/analytics/#bottleneckclassifier","title":"BottleneckClassifier","text":"<pre><code>from aaco.analytics import BottleneckClassifier\n\nclassifier = BottleneckClassifier()\nresult = classifier.classify(session)\n\nprint(result.category)    # 'compute', 'memory', 'io', etc.\nprint(result.confidence)  # 0.0 - 1.0\nprint(result.evidence)    # Supporting evidence\n</code></pre>"},{"location":"api/analytics/#categories","title":"Categories","text":"Category Description Evidence <code>compute_bound</code> GPU compute limited High SQ_BUSY, low memory BW <code>memory_bound</code> Memory bandwidth limited High L2 miss, saturated HBM <code>latency_bound</code> Memory latency limited High L2 hit but slow <code>kernel_launch</code> Launch overhead Many small kernels <code>host_bound</code> CPU/host limited CPU busy, GPU idle gaps <code>io_bound</code> Data transfer limited High PCIe utilization"},{"location":"api/analytics/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"api/analytics/#bayesianrootcause","title":"BayesianRootCause","text":"<pre><code>from aaco.analytics import BayesianRootCause\n\nanalyzer = BayesianRootCause(\n    prior=\"uniform\",  # or \"empirical\"\n    min_confidence=0.7\n)\n\nresult = analyzer.analyze(session)\n\nfor cause in result.ranked_causes:\n    print(f\"{cause.name}: {cause.posterior:.2%}\")\n    print(f\"  Evidence: {cause.evidence}\")\n</code></pre>"},{"location":"api/analytics/#root-cause-categories","title":"Root Cause Categories","text":"<pre><code>class RootCause(Enum):\n    KERNEL_INEFFICIENCY = \"kernel_inefficiency\"\n    MEMORY_ACCESS_PATTERN = \"memory_access_pattern\"\n    OCCUPANCY_LIMITED = \"occupancy_limited\"\n    REGISTER_PRESSURE = \"register_pressure\"\n    SHARED_MEMORY_BANK_CONFLICT = \"shared_memory_bank_conflict\"\n    WARP_DIVERGENCE = \"warp_divergence\"\n    LAUNCH_OVERHEAD = \"launch_overhead\"\n    HOST_DEVICE_SYNC = \"host_device_sync\"\n    DATA_TRANSFER = \"data_transfer\"\n    THERMAL_THROTTLING = \"thermal_throttling\"\n</code></pre>"},{"location":"api/analytics/#drift-detection","title":"Drift Detection","text":""},{"location":"api/analytics/#driftdetector","title":"DriftDetector","text":"<pre><code>from aaco.analytics import DriftDetector\n\ndetector = DriftDetector(\n    method=\"ewma_cusum\",\n    alpha=0.3,\n    threshold=5.0\n)\n\n# Check for drift\nresult = detector.detect(\n    baseline=baseline_session,\n    current=current_session\n)\n\nif result.has_drift:\n    print(f\"Drift detected at: {result.drift_point}\")\n    print(f\"Magnitude: {result.magnitude:.2%}\")\n</code></pre>"},{"location":"api/analytics/#methods","title":"Methods","text":"Method Description Use Case <code>ewma</code> Exponentially weighted moving average Gradual drift <code>cusum</code> Cumulative sum Step changes <code>ewma_cusum</code> Combined (default) General purpose <code>page_hinkley</code> Page-Hinkley test Online detection"},{"location":"api/analytics/#hardware-envelope","title":"Hardware Envelope","text":""},{"location":"api/analytics/#hardwareenvelopeanalyzer","title":"HardwareEnvelopeAnalyzer","text":"<pre><code>from aaco.analytics import HardwareEnvelopeAnalyzer\n\nanalyzer = HardwareEnvelopeAnalyzer(device_id=0)\n\n# Calibrate with microbenchmarks\nanalyzer.calibrate()\n\n# Analyze session\nheu = analyzer.analyze(session)\n\nprint(f\"Compute utilization: {heu.compute:.1%}\")\nprint(f\"Memory BW utilization: {heu.memory_bandwidth:.1%}\")\nprint(f\"Overall HEU: {heu.overall:.1%}\")\n</code></pre>"},{"location":"api/analytics/#attribution","title":"Attribution","text":""},{"location":"api/analytics/#kernelattributor","title":"KernelAttributor","text":"<p>Map graph operations to kernels.</p> <pre><code>from aaco.analytics import KernelAttributor\n\nattributor = KernelAttributor()\nattribution = attributor.attribute(session)\n\nfor op in attribution.operations:\n    print(f\"{op.name}:\")\n    for kernel in op.kernels:\n        print(f\"  {kernel.name}: {kernel.kar:.2f} KAR\")\n</code></pre> <p>Metrics: - KAR: Kernel Attribution Ratio - PFI: Performance Fraction Index - LTS: Latency Time Share</p>"},{"location":"api/cli/","title":"API Reference: CLI","text":"<p>The AACO command-line interface.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with the package:</p> <pre><code>pip install aaco\naaco --help\n</code></pre>"},{"location":"api/cli/#commands","title":"Commands","text":""},{"location":"api/cli/#aaco-profile","title":"aaco profile","text":"<p>Run a profiling session.</p> <pre><code>aaco profile [OPTIONS] --model MODEL\n\nOptions:\n  --model, -m PATH          Path to ONNX model (required)\n  --output, -o PATH         Output directory [default: ./sessions]\n  --iterations, -n INT      Number of iterations [default: 100]\n  --warmup, -w INT          Warmup iterations [default: 10]\n  --lab-mode, -l            Enable laboratory mode\n  --config, -c PATH         Configuration file\n  --device, -d INT          GPU device ID [default: 0]\n  --input-shapes TEXT       Input shapes as JSON\n  --collectors TEXT         Collectors to use (comma-separated)\n  --verbose, -v             Verbose output\n  --help                    Show help message\n\nExamples:\n  # Basic profile\n  aaco profile -m model.onnx\n\n  # Lab mode with custom iterations\n  aaco profile -m model.onnx -l -n 500 -w 50\n\n  # Specific collectors\n  aaco profile -m model.onnx --collectors timing,counters,traces\n</code></pre>"},{"location":"api/cli/#aaco-analyze","title":"aaco analyze","text":"<p>Analyze a profiling session.</p> <pre><code>aaco analyze [OPTIONS] --session SESSION\n\nOptions:\n  --session, -s PATH        Session directory (required)\n  --output, -o PATH         Output file\n  --format, -f TEXT         Output format [default: json]\n  --config, -c PATH         Analysis configuration\n  --verbose, -v             Verbose output\n  --help                    Show help message\n\nExamples:\n  # Basic analysis\n  aaco analyze -s sessions/20260214_120000\n\n  # Output to file\n  aaco analyze -s sessions/latest -o analysis.json\n</code></pre>"},{"location":"api/cli/#aaco-report","title":"aaco report","text":"<p>Generate reports from sessions.</p> <pre><code>aaco report [OPTIONS] --session SESSION\n\nOptions:\n  --session, -s PATH        Session directory (required)\n  --output, -o PATH         Output file\n  --format, -f TEXT         Format: html, json, markdown, pdf [default: html]\n  --template TEXT           Custom template\n  --open                    Open report after generation\n  --help                    Show help message\n\nExamples:\n  # HTML report\n  aaco report -s sessions/latest -f html -o report.html\n\n  # JSON report\n  aaco report -s sessions/latest -f json\n</code></pre>"},{"location":"api/cli/#aaco-dashboard","title":"aaco dashboard","text":"<p>Launch interactive dashboard.</p> <pre><code>aaco dashboard [OPTIONS]\n\nOptions:\n  --session, -s PATH        Session to display\n  --port, -p INT            Port number [default: 8501]\n  --host TEXT               Host address [default: localhost]\n  --no-browser              Don't open browser\n  --help                    Show help message\n\nExamples:\n  # Launch dashboard\n  aaco dashboard -s sessions/latest\n\n  # Custom port\n  aaco dashboard -s sessions/latest -p 8080\n</code></pre>"},{"location":"api/cli/#aaco-compare","title":"aaco compare","text":"<p>Compare profiling sessions.</p> <pre><code>aaco compare [OPTIONS] --baseline BASELINE --current CURRENT\n\nOptions:\n  --baseline, -b PATH       Baseline session (required)\n  --current, -c PATH        Current session (required)\n  --output, -o PATH         Output file\n  --threshold FLOAT         Regression threshold [default: 0.05]\n  --format, -f TEXT         Output format [default: table]\n  --help                    Show help message\n\nExamples:\n  # Compare sessions\n  aaco compare -b sessions/baseline -c sessions/current\n\n  # With custom threshold\n  aaco compare -b baseline -c current --threshold 0.03\n</code></pre>"},{"location":"api/cli/#aaco-doctor","title":"aaco doctor","text":"<p>System diagnostics.</p> <pre><code>aaco doctor [OPTIONS]\n\nOptions:\n  --fix                     Attempt to fix issues\n  --verbose, -v             Verbose output\n  --help                    Show help message\n\nExamples:\n  aaco doctor\n  aaco doctor --fix\n</code></pre>"},{"location":"api/cli/#configuration","title":"Configuration","text":""},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>AACO_CONFIG</code> Path to config file <code>AACO_LOG_LEVEL</code> Log level <code>AACO_SESSION_DIR</code> Default session directory <code>AACO_OUTPUT_DIR</code> Default output directory"},{"location":"api/cli/#config-file","title":"Config File","text":"<pre><code># ~/.config/aaco/config.yaml\nprofiling:\n  default_iterations: 100\n  default_warmup: 10\n\nreporting:\n  default_format: html\n</code></pre>"},{"location":"api/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 Model error 4 Hardware error 5 Regression detected (compare)"},{"location":"api/collectors/","title":"API Reference: Collectors Module","text":""},{"location":"api/collectors/#aaco.collectors","title":"collectors","text":"<p>AACO Collectors Module - System and GPU telemetry collection.</p>"},{"location":"api/collectors/#aaco.collectors.SystemSampler","title":"SystemSampler","text":"<pre><code>SystemSampler(interval_ms=200, pid=None, t0_ns=0)\n</code></pre> <p>Samples system telemetry from /proc at configurable intervals. Runs in a background thread during workload execution.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def __init__(\n    self,\n    interval_ms: int = 200,\n    pid: Optional[int] = None,\n    t0_ns: int = 0,\n):\n    self.interval_ms = interval_ms\n    self.interval_s = interval_ms / 1000.0\n    self.pid = pid or os.getpid()\n    self.t0_ns = t0_ns\n\n    self.samples: List[SystemSample] = []\n    self._running = False\n    self._thread: Optional[threading.Thread] = None\n\n    # Previous values for delta computation\n    self._prev_stat: Dict = {}\n    self._prev_vmstat: Dict = {}\n    self._prev_ctx_switches = 0\n    self._prev_majfaults = 0\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.SystemSampler.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Start background sampling thread.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start background sampling thread.\"\"\"\n    if self._running:\n        return\n\n    self._running = True\n    self._thread = threading.Thread(target=self._sample_loop, daemon=True)\n    self._thread.start()\n    logger.debug(\"System sampler started\")\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.SystemSampler.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop sampling and return collected samples.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def stop(self) -&gt; List[SystemSample]:\n    \"\"\"Stop sampling and return collected samples.\"\"\"\n    self._running = False\n    if self._thread:\n        self._thread.join(timeout=2.0)\n        self._thread = None\n\n    logger.debug(f\"System sampler stopped, collected {len(self.samples)} samples\")\n    return self.samples\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.SystemSampler.to_events","title":"to_events","text":"<pre><code>to_events()\n</code></pre> <p>Convert samples to SystemEvent schema.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def to_events(self) -&gt; List[SystemEvent]:\n    \"\"\"Convert samples to SystemEvent schema.\"\"\"\n    return [\n        SystemEvent(\n            t_ns=s.t_ns,\n            cpu_pct=s.cpu_pct,\n            rss_mb=s.rss_mb,\n            ctx_switches_delta=s.ctx_switches_delta,\n            majfault_delta=s.majfault_delta,\n            runq_len=float(s.procs_running),\n            load1=s.load1,\n            pid=self.pid,\n        )\n        for s in self.samples\n    ]\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.SystemSampler.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert samples to pandas DataFrame.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"Convert samples to pandas DataFrame.\"\"\"\n    import pandas as pd\n\n    if not self.samples:\n        return pd.DataFrame()\n\n    return pd.DataFrame(\n        [\n            {\n                \"t_ns\": s.t_ns,\n                \"t_ms\": s.t_ns / 1_000_000,\n                \"cpu_pct\": s.cpu_pct,\n                \"rss_mb\": s.rss_mb,\n                \"ctx_switches_delta\": s.ctx_switches_delta,\n                \"majfault_delta\": s.majfault_delta,\n                \"load1\": s.load1,\n                \"procs_running\": s.procs_running,\n            }\n            for s in self.samples\n        ]\n    )\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.SystemSampler.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get summary statistics of collected samples.</p> Source code in <code>aaco/collectors/sys_sampler.py</code> <pre><code>def get_summary(self) -&gt; Dict:\n    \"\"\"Get summary statistics of collected samples.\"\"\"\n    if not self.samples:\n        return {}\n\n    import numpy as np\n\n    cpu_vals = [s.cpu_pct for s in self.samples]\n    ctx_deltas = [s.ctx_switches_delta for s in self.samples]\n    majfault_deltas = [s.majfault_delta for s in self.samples]\n\n    duration_s = (\n        (self.samples[-1].t_ns - self.samples[0].t_ns) / 1e9 if len(self.samples) &gt; 1 else 1\n    )\n\n    return {\n        \"sample_count\": len(self.samples),\n        \"duration_s\": duration_s,\n        \"cpu_pct_mean\": float(np.mean(cpu_vals)),\n        \"cpu_pct_max\": float(np.max(cpu_vals)),\n        \"cpu_pct_std\": float(np.std(cpu_vals)),\n        \"ctx_switch_rate\": sum(ctx_deltas) / duration_s if duration_s &gt; 0 else 0,\n        \"majfault_rate\": sum(majfault_deltas) / duration_s if duration_s &gt; 0 else 0,\n        \"rss_mb_max\": max(s.rss_mb for s in self.samples),\n        \"load1_mean\": float(np.mean([s.load1 for s in self.samples])),\n    }\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler","title":"ROCmSMISampler","text":"<pre><code>ROCmSMISampler(interval_ms=500, device_id=0, t0_ns=0)\n</code></pre> <p>Samples GPU telemetry from rocm-smi at configurable intervals. Runs in a background thread during workload execution.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def __init__(\n    self,\n    interval_ms: int = 500,\n    device_id: int = 0,\n    t0_ns: int = 0,\n):\n    self.interval_ms = interval_ms\n    self.interval_s = interval_ms / 1000.0\n    self.device_id = device_id\n    self.t0_ns = t0_ns\n\n    self.samples: List[GPUSample] = []\n    self._running = False\n    self._thread: Optional[threading.Thread] = None\n\n    # Check if rocm-smi is available\n    self._available = self._check_rocm_smi()\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.available","title":"available  <code>property</code>","text":"<pre><code>available\n</code></pre> <p>Check if ROCm-SMI sampling is available.</p>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Start background sampling thread.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start background sampling thread.\"\"\"\n    if not self._available:\n        logger.warning(\"rocm-smi not available, GPU sampling disabled\")\n        return\n\n    if self._running:\n        return\n\n    self._running = True\n    self._thread = threading.Thread(target=self._sample_loop, daemon=True)\n    self._thread.start()\n    logger.debug(\"GPU sampler started\")\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop sampling and return collected samples.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def stop(self) -&gt; List[GPUSample]:\n    \"\"\"Stop sampling and return collected samples.\"\"\"\n    self._running = False\n    if self._thread:\n        self._thread.join(timeout=2.0)\n        self._thread = None\n\n    logger.debug(f\"GPU sampler stopped, collected {len(self.samples)} samples\")\n    return self.samples\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.to_events","title":"to_events","text":"<pre><code>to_events()\n</code></pre> <p>Convert samples to GPUEvent schema.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def to_events(self) -&gt; List[GPUEvent]:\n    \"\"\"Convert samples to GPUEvent schema.\"\"\"\n    return [\n        GPUEvent(\n            t_ns=s.t_ns,\n            gfx_clock_mhz=s.gfx_clock_mhz,\n            mem_clock_mhz=s.mem_clock_mhz,\n            power_w=s.power_w,\n            temp_c=s.temp_c,\n            vram_used_mb=s.vram_used_mb,\n            gpu_util_pct=s.gpu_util_pct,\n        )\n        for s in self.samples\n    ]\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert samples to pandas DataFrame.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"Convert samples to pandas DataFrame.\"\"\"\n    import pandas as pd\n\n    if not self.samples:\n        return pd.DataFrame()\n\n    return pd.DataFrame(\n        [\n            {\n                \"t_ns\": s.t_ns,\n                \"t_ms\": s.t_ns / 1_000_000,\n                \"gfx_clock_mhz\": s.gfx_clock_mhz,\n                \"mem_clock_mhz\": s.mem_clock_mhz,\n                \"power_w\": s.power_w,\n                \"temp_c\": s.temp_c,\n                \"vram_used_mb\": s.vram_used_mb,\n                \"gpu_util_pct\": s.gpu_util_pct,\n            }\n            for s in self.samples\n        ]\n    )\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ROCmSMISampler.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get summary statistics of collected samples.</p> Source code in <code>aaco/collectors/rocm_smi_sampler.py</code> <pre><code>def get_summary(self) -&gt; Dict:\n    \"\"\"Get summary statistics of collected samples.\"\"\"\n    if not self.samples:\n        return {}\n\n    import numpy as np\n\n    gfx_clocks = [s.gfx_clock_mhz for s in self.samples if s.gfx_clock_mhz &gt; 0]\n    powers = [s.power_w for s in self.samples if s.power_w &gt; 0]\n    temps = [s.temp_c for s in self.samples if s.temp_c &gt; 0]\n    utils = [s.gpu_util_pct for s in self.samples]\n\n    return {\n        \"sample_count\": len(self.samples),\n        \"gfx_clock_mean\": float(np.mean(gfx_clocks)) if gfx_clocks else 0,\n        \"gfx_clock_min\": float(np.min(gfx_clocks)) if gfx_clocks else 0,\n        \"gfx_clock_max\": float(np.max(gfx_clocks)) if gfx_clocks else 0,\n        \"gfx_clock_std\": float(np.std(gfx_clocks)) if gfx_clocks else 0,\n        \"power_mean\": float(np.mean(powers)) if powers else 0,\n        \"power_max\": float(np.max(powers)) if powers else 0,\n        \"temp_mean\": float(np.mean(temps)) if temps else 0,\n        \"temp_max\": float(np.max(temps)) if temps else 0,\n        \"gpu_util_mean\": float(np.mean(utils)),\n        \"gpu_util_max\": float(np.max(utils)),\n        \"vram_used_max_mb\": max(s.vram_used_mb for s in self.samples),\n    }\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.GPUSample","title":"GPUSample  <code>dataclass</code>","text":"<pre><code>GPUSample(t_ns, device_id, gfx_clock_mhz, mem_clock_mhz, power_w, temp_c, vram_used_mb, vram_total_mb, gpu_util_pct, mem_util_pct)\n</code></pre> <p>Single GPU telemetry sample.</p>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor","title":"ClockMonitor","text":"<pre><code>ClockMonitor()\n</code></pre> <p>Monitor CPU and GPU clock frequencies and power management settings.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def __init__(self):\n    self._cpu_freq_path = Path(\"/sys/devices/system/cpu\")\n    self._gpu_freq_path = Path(\"/sys/class/drm\")\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_cpu_governor","title":"get_cpu_governor","text":"<pre><code>get_cpu_governor(cpu_id=0)\n</code></pre> <p>Get CPU frequency governor.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_cpu_governor(self, cpu_id: int = 0) -&gt; str:\n    \"\"\"Get CPU frequency governor.\"\"\"\n    path = self._cpu_freq_path / f\"cpu{cpu_id}/cpufreq/scaling_governor\"\n    content = read_proc_file(str(path))\n    return content.strip() if content else \"unknown\"\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_all_cpu_governors","title":"get_all_cpu_governors","text":"<pre><code>get_all_cpu_governors()\n</code></pre> <p>Get governors for all CPUs.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_all_cpu_governors(self) -&gt; Dict[int, str]:\n    \"\"\"Get governors for all CPUs.\"\"\"\n    governors = {}\n\n    for cpu_dir in sorted(self._cpu_freq_path.glob(\"cpu[0-9]*\")):\n        try:\n            cpu_id = int(cpu_dir.name[3:])\n            governors[cpu_id] = self.get_cpu_governor(cpu_id)\n        except (ValueError, OSError):\n            continue\n\n    return governors\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_cpu_frequency","title":"get_cpu_frequency","text":"<pre><code>get_cpu_frequency(cpu_id=0)\n</code></pre> <p>Get current, min, and max CPU frequency in MHz.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_cpu_frequency(self, cpu_id: int = 0) -&gt; Dict[str, float]:\n    \"\"\"Get current, min, and max CPU frequency in MHz.\"\"\"\n    base = self._cpu_freq_path / f\"cpu{cpu_id}/cpufreq\"\n\n    freq = {\"current\": 0.0, \"min\": 0.0, \"max\": 0.0}\n\n    for key, filename in [\n        (\"current\", \"scaling_cur_freq\"),\n        (\"min\", \"scaling_min_freq\"),\n        (\"max\", \"scaling_max_freq\"),\n    ]:\n        content = read_proc_file(str(base / filename))\n        if content:\n            try:\n                # Frequency in kHz, convert to MHz\n                freq[key] = float(content.strip()) / 1000\n            except ValueError:\n                pass\n\n    return freq\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_gpu_clock_info","title":"get_gpu_clock_info","text":"<pre><code>get_gpu_clock_info(device_id=0)\n</code></pre> <p>Get GPU clock information via sysfs or rocm-smi.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_gpu_clock_info(self, device_id: int = 0) -&gt; Dict[str, float]:\n    \"\"\"Get GPU clock information via sysfs or rocm-smi.\"\"\"\n    info = {\n        \"gfx_clock_mhz\": 0.0,\n        \"mem_clock_mhz\": 0.0,\n        \"gfx_clock_min\": 0.0,\n        \"gfx_clock_max\": 0.0,\n    }\n\n    # Try sysfs first\n    card_path = self._gpu_freq_path / f\"card{device_id}/device\"\n\n    # Current GFX clock\n    content = read_proc_file(str(card_path / \"pp_dpm_sclk\"))\n    if content:\n        for line in content.split(\"\\n\"):\n            if \"*\" in line:  # Current level marked with *\n                try:\n                    match = line.split()\n                    for part in match:\n                        if \"Mhz\" in part or part.isdigit():\n                            info[\"gfx_clock_mhz\"] = float(part.replace(\"Mhz\", \"\"))\n                            break\n                except (ValueError, IndexError):\n                    pass\n\n    # Current MEM clock\n    content = read_proc_file(str(card_path / \"pp_dpm_mclk\"))\n    if content:\n        for line in content.split(\"\\n\"):\n            if \"*\" in line:\n                try:\n                    match = line.split()\n                    for part in match:\n                        if \"Mhz\" in part or part.isdigit():\n                            info[\"mem_clock_mhz\"] = float(part.replace(\"Mhz\", \"\"))\n                            break\n                except (ValueError, IndexError):\n                    pass\n\n    # Fallback to rocm-smi\n    if info[\"gfx_clock_mhz\"] == 0:\n        result = run_command([\"rocm-smi\", \"-d\", str(device_id), \"--showclocks\"])\n        if result:\n            for line in result.split(\"\\n\"):\n                if \"sclk\" in line.lower():\n                    try:\n                        parts = line.split()\n                        for i, p in enumerate(parts):\n                            if \"mhz\" in p.lower() and i &gt; 0:\n                                info[\"gfx_clock_mhz\"] = float(parts[i - 1])\n                                break\n                    except (ValueError, IndexError):\n                        pass\n\n    return info\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_gpu_power_profile","title":"get_gpu_power_profile","text":"<pre><code>get_gpu_power_profile(device_id=0)\n</code></pre> <p>Get current GPU power profile.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_gpu_power_profile(self, device_id: int = 0) -&gt; str:\n    \"\"\"Get current GPU power profile.\"\"\"\n    card_path = self._gpu_freq_path / f\"card{device_id}/device\"\n\n    content = read_proc_file(str(card_path / \"power_dpm_force_performance_level\"))\n    if content:\n        return content.strip()\n\n    # Fallback to rocm-smi\n    result = run_command([\"rocm-smi\", \"-d\", str(device_id), \"--showperflevel\"])\n    if result:\n        for line in result.split(\"\\n\"):\n            if \"performance\" in line.lower() or \"level\" in line.lower():\n                return line.strip()\n\n    return \"unknown\"\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.get_system_config","title":"get_system_config","text":"<pre><code>get_system_config()\n</code></pre> <p>Get complete clock/power configuration snapshot.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def get_system_config(self) -&gt; Dict[str, any]:\n    \"\"\"Get complete clock/power configuration snapshot.\"\"\"\n    config = {\n        \"cpu\": {\n            \"governor\": self.get_cpu_governor(0),\n            \"frequency\": self.get_cpu_frequency(0),\n            \"all_governors\": self.get_all_cpu_governors(),\n        },\n        \"gpu\": {\n            \"clocks\": self.get_gpu_clock_info(0),\n            \"power_profile\": self.get_gpu_power_profile(0),\n        },\n    }\n\n    # Check if all CPU governors are the same\n    governors = list(config[\"cpu\"][\"all_governors\"].values())\n    config[\"cpu\"][\"governors_uniform\"] = len(set(governors)) &lt;= 1\n\n    return config\n</code></pre>"},{"location":"api/collectors/#aaco.collectors.ClockMonitor.validate_performance_mode","title":"validate_performance_mode","text":"<pre><code>validate_performance_mode()\n</code></pre> <p>Validate that system is configured for consistent benchmarking. Returns dict of checks with pass/fail status.</p> Source code in <code>aaco/collectors/clocks.py</code> <pre><code>def validate_performance_mode(self) -&gt; Dict[str, bool]:\n    \"\"\"\n    Validate that system is configured for consistent benchmarking.\n    Returns dict of checks with pass/fail status.\n    \"\"\"\n    checks = {\n        \"cpu_governor_performance\": False,\n        \"gpu_power_profile_manual\": False,\n        \"governors_uniform\": False,\n    }\n\n    # CPU governor should be \"performance\"\n    governor = self.get_cpu_governor(0)\n    checks[\"cpu_governor_performance\"] = governor == \"performance\"\n\n    # All CPUs should have same governor\n    all_governors = self.get_all_cpu_governors()\n    checks[\"governors_uniform\"] = len(set(all_governors.values())) &lt;= 1\n\n    # GPU power profile should be manual/high for consistent clocks\n    power_profile = self.get_gpu_power_profile(0)\n    checks[\"gpu_power_profile_manual\"] = power_profile in [\n        \"manual\",\n        \"high\",\n        \"profile_peak\",\n    ]\n\n    return checks\n</code></pre>"},{"location":"api/collectors/#overview","title":"Overview","text":"<p>Collectors are responsible for gathering performance data during profiling sessions.</p>"},{"location":"api/collectors/#available-collectors","title":"Available Collectors","text":"Collector Description Requirements <code>TimingCollector</code> Execution time measurement None <code>CounterCollector</code> GPU hardware counters ROCm <code>TraceCollector</code> Kernel execution traces ROCm <code>MemoryCollector</code> Memory usage tracking None <code>PowerCollector</code> Power consumption ROCm <code>SystemCollector</code> System metrics None <code>EBPFCollector</code> Kernel-level metrics Linux, root"},{"location":"api/collectors/#timingcollector","title":"TimingCollector","text":"<pre><code>from aaco.collectors import TimingCollector\n\ncollector = TimingCollector(\n    precision=\"ns\",  # 'ns', 'us', 'ms'\n    clock=\"monotonic\"  # 'monotonic', 'perf_counter'\n)\n\n# Collect timing\nwith collector:\n    model.run()\n\ntiming = collector.get_results()\n</code></pre>"},{"location":"api/collectors/#countercollector","title":"CounterCollector","text":"<p>Collects GPU hardware performance counters.</p> <pre><code>from aaco.collectors import CounterCollector\n\ncollector = CounterCollector(\n    device_id=0,\n    counters=[\n        \"GRBM_COUNT\",\n        \"GRBM_GUI_ACTIVE\",\n        \"SQ_WAVES\",\n        \"TA_BUSY_CU_SUM\"\n    ]\n)\n\n# Available counter groups\nprint(CounterCollector.available_counters())\n</code></pre>"},{"location":"api/collectors/#counter-groups","title":"Counter Groups","text":"Group Counters Description <code>compute</code> SQ_, CU_ Shader/compute unit activity <code>memory</code> MC_, L2_ Memory controller, L2 cache <code>graphics</code> GRBM_, PA_ Graphics pipe activity <code>thermal</code> TEMP_, POWER_ Thermal and power"},{"location":"api/collectors/#tracecollector","title":"TraceCollector","text":"<p>Collects kernel execution traces.</p> <pre><code>from aaco.collectors import TraceCollector\n\ncollector = TraceCollector(\n    device_id=0,\n    trace_level=\"kernel\",  # 'api', 'kernel', 'full'\n    output_format=\"perfetto\"\n)\n\n# Run with tracing\ncollector.start()\nmodel.run()\ntraces = collector.stop()\n\n# Export to Perfetto\ntraces.export(\"trace.perfetto\")\n</code></pre>"},{"location":"api/collectors/#memorycollector","title":"MemoryCollector","text":"<pre><code>from aaco.collectors import MemoryCollector\n\ncollector = MemoryCollector(\n    track_gpu=True,\n    track_cpu=True,\n    sample_interval=0.01\n)\n\nmemory_data = collector.collect()\nprint(f\"Peak GPU: {memory_data.gpu_peak_mb} MB\")\nprint(f\"Peak CPU: {memory_data.cpu_peak_mb} MB\")\n</code></pre>"},{"location":"api/collectors/#powercollector","title":"PowerCollector","text":"<pre><code>from aaco.collectors import PowerCollector\n\ncollector = PowerCollector(\n    device_id=0,\n    sample_interval=0.001\n)\n\npower_data = collector.collect()\nprint(f\"Average: {power_data.average_watts} W\")\nprint(f\"Peak: {power_data.peak_watts} W\")\n</code></pre>"},{"location":"api/collectors/#ebpfcollector","title":"EBPFCollector","text":"<p>Kernel-level metrics via eBPF (requires root).</p> <pre><code>from aaco.collectors import EBPFCollector\n\ncollector = EBPFCollector(\n    probes=[\"sched\", \"irq\", \"syscall\"]\n)\n\n# Scheduler interference index\nsii = collector.scheduler_interference_index()\n\n# Context switches during measurement\ncontext_switches = collector.context_switches()\n</code></pre>"},{"location":"api/collectors/#custom-collectors","title":"Custom Collectors","text":"<p>Create custom collectors by extending <code>BaseCollector</code>:</p> <pre><code>from aaco.collectors import BaseCollector\n\nclass MyCollector(BaseCollector):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def start(self):\n        \"\"\"Start collection.\"\"\"\n        pass\n\n    def stop(self) -&gt; dict:\n        \"\"\"Stop collection and return data.\"\"\"\n        return {}\n\n    def reset(self):\n        \"\"\"Reset collector state.\"\"\"\n        pass\n</code></pre> <p>Register your collector:</p> <pre><code>from aaco import Observatory\n\nobs = Observatory()\nobs.register_collector(\"my_collector\", MyCollector)\n\nsession = obs.profile(\n    model=\"model.onnx\",\n    collectors=[\"timing\", \"my_collector\"]\n)\n</code></pre>"},{"location":"api/core/","title":"API Reference: Core Module","text":""},{"location":"api/core/#aaco.core","title":"core","text":"<p>AACO Core Module - Session management, schemas, and utilities.</p>"},{"location":"api/core/#aaco.core.Session","title":"Session","text":"<pre><code>Session(base_dir, model_name, backend, batch_size=1, session_id=None)\n</code></pre> <p>Represents a single AACO profiling session. Manages the session folder, metadata, and artifact collection.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def __init__(\n    self,\n    base_dir: Path,\n    model_name: str,\n    backend: str,\n    batch_size: int = 1,\n    session_id: Optional[str] = None,\n):\n    self.session_id = session_id or self._generate_session_id()\n    self.model_name = model_name\n    self.backend = backend\n    self.batch_size = batch_size\n\n    # Create session directory\n    date_str = datetime.now().strftime(\"%Y%m%d\")\n    self.session_dir = base_dir / date_str / self.session_id\n    self._create_directory_structure()\n\n    # Timing reference\n    self.t0_monotonic_ns = get_monotonic_ns()\n    self.t0_utc = datetime.now(timezone.utc)\n\n    # Metadata\n    self.metadata: Optional[SessionMetadata] = None\n    self._initialized = False\n</code></pre>"},{"location":"api/core/#aaco.core.Session.initialize","title":"initialize","text":"<pre><code>initialize(model_path=None, input_shapes=None, dtype='float32', warmup=10, iterations=100, ep_config=None)\n</code></pre> <p>Initialize session with full metadata. Must be called before any data collection.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def initialize(\n    self,\n    model_path: Optional[str] = None,\n    input_shapes: Optional[Dict[str, List[int]]] = None,\n    dtype: str = \"float32\",\n    warmup: int = 10,\n    iterations: int = 100,\n    ep_config: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize session with full metadata.\n    Must be called before any data collection.\n    \"\"\"\n    host_info = self._collect_host_info()\n    gpu_info = self._collect_gpu_info()\n\n    workload = WorkloadConfig(\n        framework=\"onnxruntime\",\n        model_name=self.model_name,\n        model_path=model_path or f\"models/{self.model_name}.onnx\",\n        input_shapes=input_shapes or {},\n        dtype=dtype,\n        batch_size=self.batch_size,\n        warmup_iterations=warmup,\n        measure_iterations=iterations,\n    )\n\n    backend_config = BackendConfig(\n        name=self.backend,\n        provider=self._get_ep_name(self.backend),\n        device_id=0,\n        config=ep_config or {},\n    )\n\n    self.metadata = SessionMetadata(\n        session_id=self.session_id,\n        created_utc=self.t0_utc.isoformat(),\n        t0_monotonic_ns=self.t0_monotonic_ns,\n        host=host_info,\n        gpu=gpu_info,\n        workload=workload,\n        backend=backend_config,\n    )\n\n    # Save session.json\n    self._save_session_metadata()\n\n    # Save environment lockbox\n    self._save_environment()\n\n    self._initialized = True\n</code></pre>"},{"location":"api/core/#aaco.core.Session.get_relative_time_ns","title":"get_relative_time_ns","text":"<pre><code>get_relative_time_ns()\n</code></pre> <p>Get current time relative to session start in nanoseconds.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def get_relative_time_ns(self) -&gt; int:\n    \"\"\"Get current time relative to session start in nanoseconds.\"\"\"\n    return get_monotonic_ns() - self.t0_monotonic_ns\n</code></pre>"},{"location":"api/core/#aaco.core.Session.get_artifact_path","title":"get_artifact_path","text":"<pre><code>get_artifact_path(category, filename)\n</code></pre> <p>Get the full path for an artifact file.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def get_artifact_path(self, category: str, filename: str) -&gt; Path:\n    \"\"\"Get the full path for an artifact file.\"\"\"\n    return self.session_dir / category / filename\n</code></pre>"},{"location":"api/core/#aaco.core.Session.save_artifact","title":"save_artifact","text":"<pre><code>save_artifact(category, filename, data)\n</code></pre> <p>Save an artifact (JSON, parquet, or raw).</p> Source code in <code>aaco/core/session.py</code> <pre><code>def save_artifact(self, category: str, filename: str, data: Any) -&gt; Path:\n    \"\"\"Save an artifact (JSON, parquet, or raw).\"\"\"\n    path = self.get_artifact_path(category, filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    if filename.endswith(\".json\"):\n        safe_json_dump(data, path)\n    elif filename.endswith(\".parquet\"):\n        import pandas as pd\n\n        if isinstance(data, pd.DataFrame):\n            data.to_parquet(path, index=False)\n        else:\n            pd.DataFrame(data).to_parquet(path, index=False)\n    else:\n        with open(path, \"w\") as f:\n            f.write(str(data))\n\n    return path\n</code></pre>"},{"location":"api/core/#aaco.core.Session.load_artifact","title":"load_artifact","text":"<pre><code>load_artifact(category, filename)\n</code></pre> <p>Load an artifact from the session folder.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def load_artifact(self, category: str, filename: str) -&gt; Any:\n    \"\"\"Load an artifact from the session folder.\"\"\"\n    path = self.get_artifact_path(category, filename)\n\n    if not path.exists():\n        return None\n\n    if filename.endswith(\".json\"):\n        with open(path, \"r\") as f:\n            return json.load(f)\n    elif filename.endswith(\".parquet\"):\n        import pandas as pd\n\n        return pd.read_parquet(path)\n    else:\n        with open(path, \"r\") as f:\n            return f.read()\n</code></pre>"},{"location":"api/core/#aaco.core.Session.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize session - compute final timestamps and close.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def finalize(self) -&gt; None:\n    \"\"\"Finalize session - compute final timestamps and close.\"\"\"\n    if self.metadata:\n        self.metadata.duration_s = (get_monotonic_ns() - self.t0_monotonic_ns) / 1e9\n        self._save_session_metadata()\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager","title":"SessionManager","text":"<pre><code>SessionManager(base_dir='sessions')\n</code></pre> <p>Manages multiple AACO sessions - creation, loading, and baseline comparison.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def __init__(self, base_dir: str = \"sessions\"):\n    self.base_dir = Path(base_dir)\n    self.base_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager.create_session","title":"create_session","text":"<pre><code>create_session(model_name, backend, batch_size=1, **kwargs)\n</code></pre> <p>Create a new profiling session.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def create_session(\n    self,\n    model_name: str,\n    backend: str,\n    batch_size: int = 1,\n    **kwargs,\n) -&gt; Session:\n    \"\"\"Create a new profiling session.\"\"\"\n    session = Session(\n        base_dir=self.base_dir,\n        model_name=model_name,\n        backend=backend,\n        batch_size=batch_size,\n    )\n    return session\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager.load_session","title":"load_session","text":"<pre><code>load_session(session_path)\n</code></pre> <p>Load an existing session from disk.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def load_session(self, session_path: str) -&gt; Optional[Session]:\n    \"\"\"Load an existing session from disk.\"\"\"\n    path = Path(session_path)\n\n    if not path.exists():\n        return None\n\n    session_json = path / \"session.json\"\n    if not session_json.exists():\n        return None\n\n    with open(session_json, \"r\") as f:\n        metadata = json.load(f)\n\n    # Reconstruct session\n    session = Session.__new__(Session)\n    session.session_id = metadata[\"session_id\"]\n    session.session_dir = path\n    session.model_name = metadata[\"workload\"][\"model_name\"]\n    session.backend = metadata[\"backend\"][\"name\"]\n    session.batch_size = metadata[\"workload\"][\"batch_size\"]\n    session.t0_monotonic_ns = metadata.get(\"t0_monotonic_ns\", 0)\n    session._initialized = True\n\n    return session\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager.get_latest_session","title":"get_latest_session","text":"<pre><code>get_latest_session()\n</code></pre> <p>Get the path to the most recent session.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def get_latest_session(self) -&gt; Optional[Path]:\n    \"\"\"Get the path to the most recent session.\"\"\"\n    all_sessions = []\n\n    for date_dir in self.base_dir.iterdir():\n        if date_dir.is_dir():\n            for session_dir in date_dir.iterdir():\n                if (session_dir / \"session.json\").exists():\n                    all_sessions.append(session_dir)\n\n    if not all_sessions:\n        return None\n\n    # Sort by modification time\n    all_sessions.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n    return all_sessions[0]\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager.list_sessions","title":"list_sessions","text":"<pre><code>list_sessions(model_name=None, backend=None, limit=50)\n</code></pre> <p>List sessions with optional filtering.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def list_sessions(\n    self,\n    model_name: Optional[str] = None,\n    backend: Optional[str] = None,\n    limit: int = 50,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"List sessions with optional filtering.\"\"\"\n    sessions = []\n\n    for date_dir in sorted(self.base_dir.iterdir(), reverse=True):\n        if not date_dir.is_dir():\n            continue\n\n        for session_dir in sorted(date_dir.iterdir(), reverse=True):\n            session_json = session_dir / \"session.json\"\n            if not session_json.exists():\n                continue\n\n            try:\n                with open(session_json, \"r\") as f:\n                    metadata = json.load(f)\n\n                # Apply filters\n                if model_name and metadata[\"workload\"][\"model_name\"] != model_name:\n                    continue\n                if backend and metadata[\"backend\"][\"name\"] != backend:\n                    continue\n\n                sessions.append(\n                    {\n                        \"session_id\": metadata[\"session_id\"],\n                        \"path\": str(session_dir),\n                        \"model_name\": metadata[\"workload\"][\"model_name\"],\n                        \"backend\": metadata[\"backend\"][\"name\"],\n                        \"batch_size\": metadata[\"workload\"][\"batch_size\"],\n                        \"created\": metadata[\"created_utc\"],\n                    }\n                )\n\n                if len(sessions) &gt;= limit:\n                    return sessions\n\n            except Exception:\n                continue\n\n    return sessions\n</code></pre>"},{"location":"api/core/#aaco.core.SessionManager.create_symlink_latest","title":"create_symlink_latest","text":"<pre><code>create_symlink_latest(session)\n</code></pre> <p>Create a 'latest' symlink pointing to the most recent session.</p> Source code in <code>aaco/core/session.py</code> <pre><code>def create_symlink_latest(self, session: Session) -&gt; None:\n    \"\"\"Create a 'latest' symlink pointing to the most recent session.\"\"\"\n    latest_link = self.base_dir / \"latest\"\n\n    # Remove existing symlink\n    if latest_link.exists() or latest_link.is_symlink():\n        latest_link.unlink()\n\n    # Create new symlink\n    try:\n        latest_link.symlink_to(session.session_dir)\n    except OSError:\n        # On Windows, may need admin rights for symlinks\n        pass\n</code></pre>"},{"location":"api/core/#aaco.core.BottleneckClass","title":"BottleneckClass","text":"<p>               Bases: <code>Enum</code></p> <p>Performance bottleneck classification categories.</p>"},{"location":"api/core/#aaco.core.RegressionSeverity","title":"RegressionSeverity","text":"<p>               Bases: <code>Enum</code></p> <p>Regression severity levels.</p>"},{"location":"api/core/#aaco.core.Confidence","title":"Confidence","text":"<p>               Bases: <code>Enum</code></p> <p>Confidence levels for classifications.</p>"},{"location":"api/core/#aaco.core.HostInfo","title":"HostInfo  <code>dataclass</code>","text":"<pre><code>HostInfo(hostname, os, kernel, cpu_model, ram_gb, architecture='x86_64')\n</code></pre> <p>Host system information.</p>"},{"location":"api/core/#aaco.core.GPUInfo","title":"GPUInfo  <code>dataclass</code>","text":"<pre><code>GPUInfo(vendor, name, driver, rocm_version, vram_gb, device_id=0)\n</code></pre> <p>GPU hardware information.</p>"},{"location":"api/core/#aaco.core.WorkloadConfig","title":"WorkloadConfig  <code>dataclass</code>","text":"<pre><code>WorkloadConfig(framework, model_name, model_path, input_shapes, dtype, batch_size, warmup_iterations, measure_iterations)\n</code></pre> <p>Workload configuration for inference.</p>"},{"location":"api/core/#aaco.core.BackendConfig","title":"BackendConfig  <code>dataclass</code>","text":"<pre><code>BackendConfig(name, provider, device_id, config=dict())\n</code></pre> <p>Backend/Execution Provider configuration.</p>"},{"location":"api/core/#aaco.core.SessionMetadata","title":"SessionMetadata  <code>dataclass</code>","text":"<pre><code>SessionMetadata(session_id, created_utc, t0_monotonic_ns, host, gpu, workload, backend, duration_s=0.0, notes='')\n</code></pre> <p>Complete session metadata - the spine of every session bundle.</p>"},{"location":"api/core/#aaco.core.InferenceIteration","title":"InferenceIteration  <code>dataclass</code>","text":"<pre><code>InferenceIteration(iter_idx, t_start_ns, t_end_ns, latency_ms, phase='measure', token_idx=None, decode_phase=None)\n</code></pre> <p>Single inference iteration measurement.</p>"},{"location":"api/core/#aaco.core.PhaseMetrics","title":"PhaseMetrics  <code>dataclass</code>","text":"<pre><code>PhaseMetrics(name, iterations, total_time_ms, mean_ms, std_ms, p50_ms, p90_ms, p99_ms, min_ms, max_ms, iqr_ms, cov_pct)\n</code></pre> <p>Metrics for a specific execution phase (warmup/measurement).</p>"},{"location":"api/core/#aaco.core.InferenceResult","title":"InferenceResult  <code>dataclass</code>","text":"<pre><code>InferenceResult(iterations, warmup_iterations, latencies_ms, p50_ms, p90_ms, p99_ms, mean_ms, std_ms, min_ms, max_ms, throughput_samples_per_sec, coefficient_of_variation)\n</code></pre> <p>Aggregated inference results with percentiles.</p>"},{"location":"api/core/#aaco.core.InferenceResult.from_latencies","title":"from_latencies  <code>classmethod</code>","text":"<pre><code>from_latencies(latencies, warmup=0)\n</code></pre> <p>Create InferenceResult from raw latency list.</p> Source code in <code>aaco/core/schema.py</code> <pre><code>@classmethod\ndef from_latencies(cls, latencies: List[float], warmup: int = 0) -&gt; \"InferenceResult\":\n    \"\"\"Create InferenceResult from raw latency list.\"\"\"\n    import numpy as np\n\n    arr = np.array(latencies)\n    return cls(\n        iterations=len(latencies),\n        warmup_iterations=warmup,\n        latencies_ms=latencies,\n        p50_ms=float(np.percentile(arr, 50)),\n        p90_ms=float(np.percentile(arr, 90)),\n        p99_ms=float(np.percentile(arr, 99)),\n        mean_ms=float(np.mean(arr)),\n        std_ms=float(np.std(arr)),\n        min_ms=float(np.min(arr)),\n        max_ms=float(np.max(arr)),\n        throughput_samples_per_sec=1000.0 / float(np.mean(arr)) if np.mean(arr) &gt; 0 else 0,\n        coefficient_of_variation=float(np.std(arr) / np.mean(arr)) if np.mean(arr) &gt; 0 else 0,\n    )\n</code></pre>"},{"location":"api/core/#aaco.core.SystemEvent","title":"SystemEvent  <code>dataclass</code>","text":"<pre><code>SystemEvent(t_ns, cpu_pct, rss_mb, ctx_switches_delta, majfault_delta, runq_len, load1, pid=None)\n</code></pre> <p>Single system telemetry sample.</p>"},{"location":"api/core/#aaco.core.GPUEvent","title":"GPUEvent  <code>dataclass</code>","text":"<pre><code>GPUEvent(t_ns, gfx_clock_mhz, mem_clock_mhz, power_w, temp_c, vram_used_mb, gpu_util_pct=0.0)\n</code></pre> <p>Single GPU telemetry sample.</p>"},{"location":"api/core/#aaco.core.KernelExecution","title":"KernelExecution  <code>dataclass</code>","text":"<pre><code>KernelExecution(t_start_ns, t_end_ns, dur_ns, kernel_name, queue_id=0, stream_id=0, grid_size=None, workgroup_size=None)\n</code></pre> <p>Single GPU kernel execution record from rocprof.</p>"},{"location":"api/core/#aaco.core.KernelSummary","title":"KernelSummary  <code>dataclass</code>","text":"<pre><code>KernelSummary(kernel_name, calls, total_time_ms, avg_time_us, min_time_us, max_time_us, std_time_us, pct_total)\n</code></pre> <p>Summary statistics for a GPU kernel.</p>"},{"location":"api/core/#aaco.core.KernelMetrics","title":"KernelMetrics  <code>dataclass</code>","text":"<pre><code>KernelMetrics(total_kernel_count, unique_kernel_count, total_kernel_time_ms, avg_kernel_duration_us, microkernel_count, microkernel_pct, microkernel_threshold_us, launch_rate_per_sec, launch_tax_score, kernel_amplification_ratio, gpu_active_ratio, top_kernels=list())\n</code></pre> <p>Derived kernel-level metrics for bottleneck analysis.</p>"},{"location":"api/core/#aaco.core.GraphNode","title":"GraphNode  <code>dataclass</code>","text":"<pre><code>GraphNode(node_id, node_name, op_type, domain, inputs, outputs, input_shapes, output_shapes, attributes=dict(), estimated_flops=None, estimated_bytes=None)\n</code></pre> <p>ONNX graph node metadata.</p>"},{"location":"api/core/#aaco.core.KernelAttribution","title":"KernelAttribution  <code>dataclass</code>","text":"<pre><code>KernelAttribution(node_id, op_type, partition_id, kernel_group_id, kernel_names, attribution_method, confidence, evidence=dict())\n</code></pre> <p>Mapping from ONNX node to GPU kernel group.</p>"},{"location":"api/core/#aaco.core.EvidenceSignal","title":"EvidenceSignal  <code>dataclass</code>","text":"<pre><code>EvidenceSignal(signal_name, value, weight, threshold=None, direction='higher_is_worse')\n</code></pre> <p>Single evidence signal for bottleneck classification.</p>"},{"location":"api/core/#aaco.core.BottleneckClassification","title":"BottleneckClassification  <code>dataclass</code>","text":"<pre><code>BottleneckClassification(bottleneck_class, confidence, score, top_evidence, explanation, recommendations)\n</code></pre> <p>Bottleneck classification result with evidence.</p>"},{"location":"api/core/#aaco.core.MetricDelta","title":"MetricDelta  <code>dataclass</code>","text":"<pre><code>MetricDelta(metric_name, baseline_value, current_value, delta_absolute, delta_pct, significant, direction)\n</code></pre> <p>Delta between two metrics for regression detection.</p>"},{"location":"api/core/#aaco.core.RegressionVerdict","title":"RegressionVerdict  <code>dataclass</code>","text":"<pre><code>RegressionVerdict(regression, severity, confidence, latency_delta_pct, suspected_cause, key_deltas, evidence, recommendation, baseline_session_id, comparison_session_id)\n</code></pre> <p>Final regression verdict with root cause analysis.</p>"},{"location":"api/core/#aaco.core.DerivedMetrics","title":"DerivedMetrics  <code>dataclass</code>","text":"<pre><code>DerivedMetrics(warmup_phase, measurement_phase, throughput, efficiency, latency, system, gpu)\n</code></pre> <p>All derived performance metrics for a session.</p>"},{"location":"api/core/#aaco.core.Timer","title":"Timer","text":"<pre><code>Timer(name='')\n</code></pre> <p>Context manager for timing code blocks.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def __init__(self, name: str = \"\"):\n    self.name = name\n    self.start_ns = 0\n    self.end_ns = 0\n    self.duration_ns = 0\n    self.duration_ms = 0.0\n</code></pre>"},{"location":"api/core/#aaco.core.RateTracker","title":"RateTracker","text":"<pre><code>RateTracker(window_seconds=1.0)\n</code></pre> <p>Track rates over a sliding time window.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def __init__(self, window_seconds: float = 1.0):\n    self.window_ns = int(window_seconds * 1e9)\n    self.events: List[int] = []\n</code></pre>"},{"location":"api/core/#aaco.core.RateTracker.record","title":"record","text":"<pre><code>record(timestamp_ns=None)\n</code></pre> <p>Record an event.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def record(self, timestamp_ns: Optional[int] = None) -&gt; None:\n    \"\"\"Record an event.\"\"\"\n    ts = timestamp_ns or get_monotonic_ns()\n    self.events.append(ts)\n    self._cleanup(ts)\n</code></pre>"},{"location":"api/core/#aaco.core.RateTracker.get_rate","title":"get_rate","text":"<pre><code>get_rate()\n</code></pre> <p>Get events per second.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def get_rate(self) -&gt; float:\n    \"\"\"Get events per second.\"\"\"\n    if not self.events:\n        return 0.0\n\n    current = get_monotonic_ns()\n    self._cleanup(current)\n\n    if not self.events:\n        return 0.0\n\n    window_actual = (current - self.events[0]) / 1e9\n    if window_actual &lt;= 0:\n        return 0.0\n\n    return len(self.events) / window_actual\n</code></pre>"},{"location":"api/core/#aaco.core.get_monotonic_ns","title":"get_monotonic_ns","text":"<pre><code>get_monotonic_ns()\n</code></pre> <p>Get monotonic clock time in nanoseconds.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def get_monotonic_ns() -&gt; int:\n    \"\"\"Get monotonic clock time in nanoseconds.\"\"\"\n    return time.monotonic_ns()\n</code></pre>"},{"location":"api/core/#aaco.core.get_monotonic_ms","title":"get_monotonic_ms","text":"<pre><code>get_monotonic_ms()\n</code></pre> <p>Get monotonic clock time in milliseconds.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def get_monotonic_ms() -&gt; float:\n    \"\"\"Get monotonic clock time in milliseconds.\"\"\"\n    return time.monotonic_ns() / 1_000_000\n</code></pre>"},{"location":"api/core/#aaco.core.ns_to_ms","title":"ns_to_ms","text":"<pre><code>ns_to_ms(ns)\n</code></pre> <p>Convert nanoseconds to milliseconds.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def ns_to_ms(ns: int) -&gt; float:\n    \"\"\"Convert nanoseconds to milliseconds.\"\"\"\n    return ns / 1_000_000\n</code></pre>"},{"location":"api/core/#aaco.core.ns_to_us","title":"ns_to_us","text":"<pre><code>ns_to_us(ns)\n</code></pre> <p>Convert nanoseconds to microseconds.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def ns_to_us(ns: int) -&gt; float:\n    \"\"\"Convert nanoseconds to microseconds.\"\"\"\n    return ns / 1_000\n</code></pre>"},{"location":"api/core/#aaco.core.ms_to_ns","title":"ms_to_ns","text":"<pre><code>ms_to_ns(ms)\n</code></pre> <p>Convert milliseconds to nanoseconds.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def ms_to_ns(ms: float) -&gt; int:\n    \"\"\"Convert milliseconds to nanoseconds.\"\"\"\n    return int(ms * 1_000_000)\n</code></pre>"},{"location":"api/core/#aaco.core.run_command","title":"run_command","text":"<pre><code>run_command(cmd, timeout=30, cwd=None, env=None, capture_stderr=True)\n</code></pre> <p>Run a shell command and return stdout. Returns None on failure.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def run_command(\n    cmd: List[str],\n    timeout: int = 30,\n    cwd: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n    capture_stderr: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Run a shell command and return stdout.\n    Returns None on failure.\n    \"\"\"\n    try:\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            cwd=cwd,\n            env=env or os.environ.copy(),\n        )\n        if result.returncode == 0:\n            return result.stdout\n        else:\n            if capture_stderr:\n                logger.debug(f\"Command failed: {cmd}\\nstderr: {result.stderr}\")\n            return None\n    except subprocess.TimeoutExpired:\n        logger.warning(f\"Command timed out: {cmd}\")\n        return None\n    except FileNotFoundError:\n        logger.debug(f\"Command not found: {cmd[0]}\")\n        return None\n    except Exception as e:\n        logger.debug(f\"Command error: {cmd}, {e}\")\n        return None\n</code></pre>"},{"location":"api/core/#aaco.core.run_command_async","title":"run_command_async","text":"<pre><code>run_command_async(cmd, cwd=None, env=None)\n</code></pre> <p>Start a command asynchronously and return the Popen object.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def run_command_async(\n    cmd: List[str],\n    cwd: Optional[str] = None,\n    env: Optional[Dict[str, str]] = None,\n) -&gt; subprocess.Popen:\n    \"\"\"Start a command asynchronously and return the Popen object.\"\"\"\n    return subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=cwd,\n        env=env or os.environ.copy(),\n    )\n</code></pre>"},{"location":"api/core/#aaco.core.safe_json_dump","title":"safe_json_dump","text":"<pre><code>safe_json_dump(data, path, indent=2)\n</code></pre> <p>Safely write JSON to file with atomic write pattern.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def safe_json_dump(data: Any, path: Union[str, Path], indent: int = 2) -&gt; None:\n    \"\"\"Safely write JSON to file with atomic write pattern.\"\"\"\n    path = Path(path)\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        with open(temp_path, \"w\") as f:\n            json.dump(data, f, indent=indent, default=str)\n        temp_path.replace(path)\n    except Exception as e:\n        logger.error(f\"Failed to write JSON to {path}: {e}\")\n        if temp_path.exists():\n            temp_path.unlink()\n        raise\n</code></pre>"},{"location":"api/core/#aaco.core.safe_json_load","title":"safe_json_load","text":"<pre><code>safe_json_load(path)\n</code></pre> <p>Safely load JSON from file.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def safe_json_load(path: Union[str, Path]) -&gt; Optional[Any]:\n    \"\"\"Safely load JSON from file.\"\"\"\n    try:\n        with open(path, \"r\") as f:\n            return json.load(f)\n    except Exception as e:\n        logger.error(f\"Failed to load JSON from {path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/core/#aaco.core.read_proc_file","title":"read_proc_file","text":"<pre><code>read_proc_file(path)\n</code></pre> <p>Read a /proc or /sys file, return None on failure.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def read_proc_file(path: str) -&gt; Optional[str]:\n    \"\"\"Read a /proc or /sys file, return None on failure.\"\"\"\n    try:\n        with open(path, \"r\") as f:\n            return f.read()\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/core/#aaco.core.read_proc_stat","title":"read_proc_stat","text":"<pre><code>read_proc_stat()\n</code></pre> <p>Read /proc/stat and parse CPU statistics.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def read_proc_stat() -&gt; Dict[str, Any]:\n    \"\"\"Read /proc/stat and parse CPU statistics.\"\"\"\n    content = read_proc_file(\"/proc/stat\")\n    if not content:\n        return {}\n\n    stats = {}\n    for line in content.split(\"\\n\"):\n        if line.startswith(\"cpu \"):\n            parts = line.split()\n            stats[\"cpu_total\"] = {\n                \"user\": int(parts[1]),\n                \"nice\": int(parts[2]),\n                \"system\": int(parts[3]),\n                \"idle\": int(parts[4]),\n                \"iowait\": int(parts[5]) if len(parts) &gt; 5 else 0,\n                \"irq\": int(parts[6]) if len(parts) &gt; 6 else 0,\n                \"softirq\": int(parts[7]) if len(parts) &gt; 7 else 0,\n            }\n        elif line.startswith(\"ctxt\"):\n            stats[\"context_switches\"] = int(line.split()[1])\n        elif line.startswith(\"procs_running\"):\n            stats[\"procs_running\"] = int(line.split()[1])\n        elif line.startswith(\"procs_blocked\"):\n            stats[\"procs_blocked\"] = int(line.split()[1])\n\n    return stats\n</code></pre>"},{"location":"api/core/#aaco.core.read_proc_vmstat","title":"read_proc_vmstat","text":"<pre><code>read_proc_vmstat()\n</code></pre> <p>Read /proc/vmstat and parse memory statistics.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def read_proc_vmstat() -&gt; Dict[str, int]:\n    \"\"\"Read /proc/vmstat and parse memory statistics.\"\"\"\n    content = read_proc_file(\"/proc/vmstat\")\n    if not content:\n        return {}\n\n    stats = {}\n    for line in content.split(\"\\n\"):\n        parts = line.split()\n        if len(parts) == 2:\n            try:\n                stats[parts[0]] = int(parts[1])\n            except ValueError:\n                pass\n\n    return stats\n</code></pre>"},{"location":"api/core/#aaco.core.read_loadavg","title":"read_loadavg","text":"<pre><code>read_loadavg()\n</code></pre> <p>Read /proc/loadavg.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def read_loadavg() -&gt; Dict[str, float]:\n    \"\"\"Read /proc/loadavg.\"\"\"\n    content = read_proc_file(\"/proc/loadavg\")\n    if not content:\n        return {}\n\n    parts = content.split()\n    return {\n        \"load1\": float(parts[0]),\n        \"load5\": float(parts[1]),\n        \"load15\": float(parts[2]),\n    }\n</code></pre>"},{"location":"api/core/#aaco.core.read_process_stat","title":"read_process_stat","text":"<pre><code>read_process_stat(pid)\n</code></pre> <p>Read /proc//stat for process-specific statistics. Source code in <code>aaco/core/utils.py</code> <pre><code>def read_process_stat(pid: int) -&gt; Dict[str, Any]:\n    \"\"\"Read /proc/&lt;pid&gt;/stat for process-specific statistics.\"\"\"\n    content = read_proc_file(f\"/proc/{pid}/stat\")\n    if not content:\n        return {}\n\n    # Parse the complex /proc/pid/stat format\n    # Format: pid (comm) state ppid pgrp session tty_nr tpgid flags minflt cminflt majflt cmajflt utime stime ...\n    parts = content.split()\n\n    return {\n        \"pid\": int(parts[0]),\n        \"state\": parts[2],\n        \"minflt\": int(parts[9]),\n        \"majflt\": int(parts[11]),\n        \"utime\": int(parts[13]),\n        \"stime\": int(parts[14]),\n        \"num_threads\": int(parts[19]) if len(parts) &gt; 19 else 1,\n        \"vsize\": int(parts[22]) if len(parts) &gt; 22 else 0,\n        \"rss\": int(parts[23]) if len(parts) &gt; 23 else 0,\n    }\n</code></pre>"},{"location":"api/core/#aaco.core.read_process_status","title":"read_process_status","text":"<pre><code>read_process_status(pid)\n</code></pre> <p>Read /proc//status for detailed process info. Source code in <code>aaco/core/utils.py</code> <pre><code>def read_process_status(pid: int) -&gt; Dict[str, Any]:\n    \"\"\"Read /proc/&lt;pid&gt;/status for detailed process info.\"\"\"\n    content = read_proc_file(f\"/proc/{pid}/status\")\n    if not content:\n        return {}\n\n    status = {}\n    for line in content.split(\"\\n\"):\n        if \":\" in line:\n            key, value = line.split(\":\", 1)\n            key = key.strip()\n            value = value.strip()\n\n            # Parse specific fields\n            if key in [\"VmRSS\", \"VmSize\", \"VmPeak\"]:\n                # Remove \"kB\" suffix and convert\n                try:\n                    status[key] = int(value.split()[0])\n                except (ValueError, IndexError):\n                    pass\n            elif key in [\"voluntary_ctxt_switches\", \"nonvoluntary_ctxt_switches\"]:\n                try:\n                    status[key] = int(value)\n                except ValueError:\n                    pass\n            elif key == \"Threads\":\n                try:\n                    status[\"threads\"] = int(value)\n                except ValueError:\n                    pass\n\n    return status\n</code></pre>"},{"location":"api/core/#aaco.core.compute_cpu_percent","title":"compute_cpu_percent","text":"<pre><code>compute_cpu_percent(prev_stat, curr_stat)\n</code></pre> <p>Compute CPU percentage between two /proc/stat snapshots.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def compute_cpu_percent(prev_stat: Dict, curr_stat: Dict) -&gt; float:\n    \"\"\"Compute CPU percentage between two /proc/stat snapshots.\"\"\"\n    if not prev_stat or not curr_stat:\n        return 0.0\n\n    prev_cpu = prev_stat.get(\"cpu_total\", {})\n    curr_cpu = curr_stat.get(\"cpu_total\", {})\n\n    if not prev_cpu or not curr_cpu:\n        return 0.0\n\n    prev_idle = prev_cpu.get(\"idle\", 0) + prev_cpu.get(\"iowait\", 0)\n    curr_idle = curr_cpu.get(\"idle\", 0) + curr_cpu.get(\"iowait\", 0)\n\n    prev_total = sum(prev_cpu.values())\n    curr_total = sum(curr_cpu.values())\n\n    total_diff = curr_total - prev_total\n    idle_diff = curr_idle - prev_idle\n\n    if total_diff == 0:\n        return 0.0\n\n    return 100.0 * (1.0 - idle_diff / total_diff)\n</code></pre>"},{"location":"api/core/#aaco.core.format_size","title":"format_size","text":"<pre><code>format_size(bytes)\n</code></pre> <p>Format bytes as human-readable string.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def format_size(bytes: int) -&gt; str:\n    \"\"\"Format bytes as human-readable string.\"\"\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if abs(bytes) &lt; 1024.0:\n            return f\"{bytes:.1f} {unit}\"\n        bytes /= 1024.0\n    return f\"{bytes:.1f} PB\"\n</code></pre>"},{"location":"api/core/#aaco.core.format_duration","title":"format_duration","text":"<pre><code>format_duration(seconds)\n</code></pre> <p>Format duration as human-readable string.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def format_duration(seconds: float) -&gt; str:\n    \"\"\"Format duration as human-readable string.\"\"\"\n    if seconds &lt; 0.001:\n        return f\"{seconds * 1_000_000:.1f} \u00b5s\"\n    elif seconds &lt; 1:\n        return f\"{seconds * 1000:.2f} ms\"\n    elif seconds &lt; 60:\n        return f\"{seconds:.2f} s\"\n    elif seconds &lt; 3600:\n        return f\"{seconds / 60:.1f} min\"\n    else:\n        return f\"{seconds / 3600:.1f} h\"\n</code></pre>"},{"location":"api/core/#aaco.core.percentile","title":"percentile","text":"<pre><code>percentile(data, p)\n</code></pre> <p>Calculate percentile of a list of values.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def percentile(data: List[float], p: float) -&gt; float:\n    \"\"\"Calculate percentile of a list of values.\"\"\"\n    if not data:\n        return 0.0\n\n    import numpy as np\n\n    return float(np.percentile(data, p))\n</code></pre>"},{"location":"api/core/#aaco.core.ensure_dir","title":"ensure_dir","text":"<pre><code>ensure_dir(path)\n</code></pre> <p>Ensure directory exists, create if needed.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def ensure_dir(path: Union[str, Path]) -&gt; Path:\n    \"\"\"Ensure directory exists, create if needed.\"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n</code></pre>"},{"location":"api/core/#aaco.core.hash_file","title":"hash_file","text":"<pre><code>hash_file(path, algorithm='sha256')\n</code></pre> <p>Compute hash of a file.</p> Source code in <code>aaco/core/utils.py</code> <pre><code>def hash_file(path: Union[str, Path], algorithm: str = \"sha256\") -&gt; str:\n    \"\"\"Compute hash of a file.\"\"\"\n    import hashlib\n\n    h = hashlib.new(algorithm)\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()[:16]\n</code></pre>"},{"location":"api/core/#observatory","title":"Observatory","text":"<p>The main entry point for AACO functionality.</p> <pre><code>from aaco import Observatory\n\nobs = Observatory(config=None)\n</code></pre>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#profile","title":"profile()","text":"<p>Run a profiling session.</p> <pre><code>def profile(\n    self,\n    model: str | Path,\n    iterations: int = 100,\n    warmup: int = 10,\n    lab_mode: bool = False,\n    config: dict = None,\n    **kwargs\n) -&gt; Session:\n    \"\"\"\n    Profile a model and return session data.\n\n    Args:\n        model: Path to ONNX model or model identifier\n        iterations: Number of measured iterations\n        warmup: Number of warmup iterations\n        lab_mode: Enable laboratory mode\n        config: Additional configuration\n\n    Returns:\n        Session object containing profiling data\n\n    Example:\n        &gt;&gt;&gt; obs = Observatory()\n        &gt;&gt;&gt; session = obs.profile(\"model.onnx\", iterations=100)\n    \"\"\"\n</code></pre>"},{"location":"api/core/#analyze","title":"analyze()","text":"<p>Analyze a profiling session.</p> <pre><code>def analyze(\n    self,\n    session: Session | str | Path,\n    config: dict = None\n) -&gt; Analysis:\n    \"\"\"\n    Analyze profiling session data.\n\n    Args:\n        session: Session object or path to session\n        config: Analysis configuration\n\n    Returns:\n        Analysis object with results\n    \"\"\"\n</code></pre>"},{"location":"api/core/#report","title":"report()","text":"<p>Generate a report from session data.</p> <pre><code>def report(\n    self,\n    session: Session | str | Path,\n    format: str = \"html\",\n    output: str | Path = None,\n    **kwargs\n) -&gt; Path:\n    \"\"\"\n    Generate a report from session data.\n\n    Args:\n        session: Session to report on\n        format: Output format ('html', 'json', 'markdown', 'pdf')\n        output: Output file path\n\n    Returns:\n        Path to generated report\n    \"\"\"\n</code></pre>"},{"location":"api/core/#compare","title":"compare()","text":"<p>Compare two sessions.</p> <pre><code>def compare(\n    self,\n    baseline: Session | str | Path,\n    current: Session | str | Path,\n    config: dict = None\n) -&gt; Comparison:\n    \"\"\"\n    Compare two profiling sessions.\n\n    Args:\n        baseline: Baseline session\n        current: Current session to compare\n        config: Comparison configuration\n\n    Returns:\n        Comparison results\n    \"\"\"\n</code></pre>"},{"location":"api/core/#session","title":"Session","text":"<p>Represents a complete profiling session.</p> <pre><code>@dataclass\nclass Session:\n    \"\"\"\n    A profiling session containing all collected data.\n\n    Attributes:\n        id: Unique session identifier\n        model: Model information\n        config: Session configuration\n        metrics: Collected metrics\n        traces: Execution traces\n        metadata: Session metadata\n    \"\"\"\n\n    id: str\n    model: ModelInfo\n    config: SessionConfig\n    metrics: Metrics\n    traces: TraceData\n    metadata: dict\n</code></pre>"},{"location":"api/core/#properties","title":"Properties","text":"Property Type Description <code>latency</code> <code>Series</code> Latency measurements <code>throughput</code> <code>float</code> Throughput (inferences/sec) <code>memory_usage</code> <code>dict</code> Memory statistics <code>gpu_utilization</code> <code>float</code> GPU utilization %"},{"location":"api/core/#analysis","title":"Analysis","text":"<p>Analysis results from a session.</p> <pre><code>@dataclass\nclass Analysis:\n    \"\"\"\n    Analysis results containing insights and metrics.\n\n    Attributes:\n        session: Source session\n        statistics: Statistical summary\n        bottleneck: Bottleneck classification\n        heu: Hardware envelope utilization\n    \"\"\"\n</code></pre>"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#root_cause","title":"root_cause()","text":"<pre><code>def root_cause(self, min_confidence: float = 0.7) -&gt; RootCauseResult:\n    \"\"\"\n    Perform Bayesian root cause analysis.\n\n    Returns:\n        RootCauseResult with ranked causes\n    \"\"\"\n</code></pre>"},{"location":"api/core/#classify_bottleneck","title":"classify_bottleneck()","text":"<pre><code>def classify_bottleneck(self) -&gt; BottleneckResult:\n    \"\"\"\n    Classify the performance bottleneck.\n\n    Returns:\n        BottleneckResult with category and evidence\n    \"\"\"\n</code></pre>"},{"location":"api/core/#config","title":"Config","text":"<p>Configuration management.</p> <pre><code>from aaco import Config\n\n# Load default config\nconfig = Config.default()\n\n# Load from file\nconfig = Config.from_file(\"aaco.yaml\")\n\n# Load from dict\nconfig = Config.from_dict({...})\n</code></pre>"},{"location":"api/core/#exceptions","title":"Exceptions","text":"<pre><code>from aaco.core.exceptions import (\n    AACOError,           # Base exception\n    ProfileError,        # Profiling errors\n    AnalysisError,       # Analysis errors\n    ConfigError,         # Configuration errors\n    ModelError,          # Model loading errors\n    HardwareError,       # Hardware access errors\n)\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>AACO uses YAML configuration files for customization.</p>"},{"location":"getting-started/configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<p>AACO searches for configuration in this order:</p> <ol> <li>Command-line specified: <code>--config path/to/config.yaml</code></li> <li>Current directory: <code>./aaco.yaml</code></li> <li>User config: <code>~/.config/aaco/config.yaml</code></li> <li>System config: <code>/etc/aaco/config.yaml</code></li> </ol>"},{"location":"getting-started/configuration/#default-configuration","title":"Default Configuration","text":"<pre><code># aaco.yaml - AACO Configuration\n# \u00a9 2026 Sudheer Ibrahim Daniel Devu\n\n# =============================================================================\n# General Settings\n# =============================================================================\ngeneral:\n  log_level: INFO\n  session_dir: ./sessions\n  output_dir: ./results\n  temp_dir: /tmp/aaco\n\n# =============================================================================\n# Profiling Settings\n# =============================================================================\nprofiling:\n  # Iteration settings\n  default_iterations: 100\n  default_warmup: 10\n  min_iterations: 10\n  max_iterations: 10000\n\n  # Timing\n  timeout: 3600  # seconds\n  sample_interval: 0.001  # seconds\n\n  # Data collection\n  collect_counters: true\n  collect_traces: true\n  collect_memory: true\n  collect_power: true\n\n# =============================================================================\n# Laboratory Mode\n# =============================================================================\nlaboratory:\n  enabled: false\n\n  # CPU isolation\n  cpu_isolation:\n    enabled: true\n    cores: [4, 5, 6, 7]  # Isolated cores to use\n    pin_threads: true\n\n  # GPU settings\n  gpu:\n    clock_lock: true\n    target_frequency: max  # 'max', 'min', or specific MHz\n    power_limit: null  # Watts, null for default\n\n  # Process isolation\n  process:\n    nice: -20\n    ionice_class: realtime\n    cgroup_isolation: true\n\n  # System preparation\n  system:\n    disable_turbo: true\n    set_governor: performance\n    drop_caches: true\n\n# =============================================================================\n# Analysis Settings\n# =============================================================================\nanalysis:\n  # Statistical settings\n  statistics:\n    confidence_level: 0.95\n    outlier_method: iqr  # 'iqr', 'zscore', 'mad'\n    outlier_threshold: 1.5\n\n  # Baseline settings\n  baseline:\n    method: median  # 'mean', 'median', 'trimmed_mean'\n    robust: true\n\n  # Drift detection\n  drift:\n    ewma_alpha: 0.3\n    cusum_threshold: 5.0\n    window_size: 20\n\n  # Root cause\n  root_cause:\n    prior_method: uniform  # 'uniform', 'empirical'\n    min_confidence: 0.7\n    max_causes: 5\n\n# =============================================================================\n# Hardware Settings\n# =============================================================================\nhardware:\n  # GPU detection\n  gpu:\n    auto_detect: true\n    device_ids: null  # null for all, or [0, 1, ...]\n\n  # ROCm settings\n  rocm:\n    path: /opt/rocm\n    version: auto\n\n  # MIGraphX settings\n  migraphx:\n    exhaustive_tune: false\n    fast_math: true\n\n# =============================================================================\n# Report Settings\n# =============================================================================\nreporting:\n  # Output formats\n  default_format: html\n\n  # HTML report\n  html:\n    theme: auto  # 'light', 'dark', 'auto'\n    interactive: true\n    include_raw_data: false\n\n  # JSON report\n  json:\n    pretty: true\n    include_session: true\n\n  # Charts\n  charts:\n    style: seaborn\n    dpi: 150\n    figsize: [12, 8]\n\n# =============================================================================\n# Dashboard Settings\n# =============================================================================\ndashboard:\n  host: localhost\n  port: 8501\n  theme: dark\n  auto_refresh: true\n  refresh_interval: 5  # seconds\n\n# =============================================================================\n# Storage Settings\n# =============================================================================\nstorage:\n  # Session storage\n  session:\n    format: parquet  # 'parquet', 'json', 'pickle'\n    compression: zstd\n\n  # Data retention\n  retention:\n    max_sessions: 100\n    max_age_days: 30\n    auto_cleanup: true\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Override configuration with environment variables:</p> Variable Description <code>AACO_CONFIG</code> Path to config file <code>AACO_LOG_LEVEL</code> Log level (DEBUG, INFO, WARNING, ERROR) <code>AACO_SESSION_DIR</code> Session storage directory <code>AACO_OUTPUT_DIR</code> Output directory <code>AACO_LAB_MODE</code> Enable laboratory mode (1/0) <code>ROCM_PATH</code> ROCm installation path"},{"location":"getting-started/configuration/#configuration-profiles","title":"Configuration Profiles","text":""},{"location":"getting-started/configuration/#performance-profile","title":"Performance Profile","text":"<pre><code># High-accuracy profiling\nprofiling:\n  default_iterations: 500\n  default_warmup: 50\n\nlaboratory:\n  enabled: true\n  cpu_isolation:\n    enabled: true\n  gpu:\n    clock_lock: true\n</code></pre>"},{"location":"getting-started/configuration/#quick-profile","title":"Quick Profile","text":"<pre><code># Fast profiling for development\nprofiling:\n  default_iterations: 20\n  default_warmup: 5\n\nlaboratory:\n  enabled: false\n\nanalysis:\n  statistics:\n    outlier_method: none\n</code></pre>"},{"location":"getting-started/configuration/#ci-profile","title":"CI Profile","text":"<pre><code># CI/CD optimized\nprofiling:\n  default_iterations: 50\n  timeout: 600\n\nreporting:\n  default_format: json\n  json:\n    include_session: false\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Laboratory Mode - Deterministic profiling</li> <li>Analysis Guide - Understanding results</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers installing AMD AI Compute Observatory on your system.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":""},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"Requirement Minimum Recommended Python 3.10+ 3.11 OS Linux (Ubuntu 20.04+) Ubuntu 22.04 LTS RAM 8 GB 16 GB+ GPU AMD ROCm-compatible MI100/MI200/MI300 ROCm 5.7+ 6.0+"},{"location":"getting-started/installation/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>Python 3.10 or higher</li> <li>ROCm 6.0+ (for GPU profiling features)</li> <li>Linux kernel 5.15+ (for eBPF features)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<pre><code># Basic installation\npip install aaco\n\n# With all optional dependencies\npip install aaco[all]\n\n# With specific extras\npip install aaco[onnx,dashboard,ml]\n</code></pre>"},{"location":"getting-started/installation/#method-2-from-source","title":"Method 2: From Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/SID-Devu/AMD-AI-Compute-Observatory.git\ncd AMD-AI-Compute-Observatory\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker","title":"Method 3: Docker","text":"<pre><code># Pull the image\ndocker pull ghcr.io/sid-devu/amd-ai-compute-observatory:latest\n\n# Or build locally\ndocker build -t aaco:latest .\n\n# Run\ndocker run -it aaco:latest --help\n</code></pre>"},{"location":"getting-started/installation/#method-4-docker-compose","title":"Method 4: Docker Compose","text":"<pre><code># Start all services\ndocker-compose up -d\n\n# Start specific services\ndocker-compose up -d dashboard jupyter\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Extra Packages Use Case <code>onnx</code> onnx, onnxruntime ONNX model support <code>dashboard</code> streamlit, plotly Interactive dashboard <code>ml</code> scikit-learn, shap ML-powered analysis <code>ebpf</code> bcc Kernel-level profiling <code>dev</code> pytest, mypy, ruff Development tools <code>all</code> All of the above Full installation"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code># Check CLI\naaco --version\n\n# Run self-test\naaco doctor\n\n# Test import\npython -c \"import aaco; print(aaco.__version__)\"\n</code></pre>"},{"location":"getting-started/installation/#rocm-setup","title":"ROCm Setup","text":"<p>For GPU profiling features, ensure ROCm is properly installed:</p> <pre><code># Check ROCm installation\nrocm-smi\n\n# Verify HIP\nhipcc --version\n\n# Check MIGraphX\nmigraphx-driver --version\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"Import Error: No module named 'aaco' <p>Ensure you've activated your virtual environment: <pre><code>source venv/bin/activate\n</code></pre></p> ROCm not detected <p>Check ROCm installation: <pre><code>ls /opt/rocm\nrocm-smi --showproductname\n</code></pre></p> Permission denied for eBPF <p>eBPF features require root or CAP_BPF: <pre><code>sudo aaco profile --ebpf ...\n# Or add capabilities\nsudo setcap cap_bpf+ep $(which python)\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Configuration - Configure AACO for your environment</li> <li>User Guide - Complete usage documentation</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with AACO in 5 minutes!</p>"},{"location":"getting-started/quickstart/#basic-profiling","title":"Basic Profiling","text":""},{"location":"getting-started/quickstart/#profile-an-onnx-model","title":"Profile an ONNX Model","text":"<pre><code># Basic profile\naaco profile --model resnet50.onnx --output results/\n\n# With more iterations for statistical significance\naaco profile --model resnet50.onnx --iterations 100 --warmup 10\n\n# Laboratory mode for deterministic results\naaco profile --model resnet50.onnx --lab-mode --output results/lab/\n</code></pre>"},{"location":"getting-started/quickstart/#view-results","title":"View Results","text":"<pre><code># Generate report\naaco report --session results/latest --format html\n\n# Launch interactive dashboard\naaco dashboard --session results/latest\n\n# Quick summary\naaco summary --session results/latest\n</code></pre>"},{"location":"getting-started/quickstart/#cli-overview","title":"CLI Overview","text":"<pre><code># Get help\naaco --help\n\n# Available commands\naaco profile    # Run profiling session\naaco analyze    # Analyze profiling data\naaco report     # Generate reports\naaco dashboard  # Launch interactive dashboard\naaco compare    # Compare multiple sessions\naaco doctor     # System diagnostics\n</code></pre>"},{"location":"getting-started/quickstart/#python-api","title":"Python API","text":"<pre><code>from aaco import Observatory, Config\n\n# Create observatory instance\nobs = Observatory(config=Config.default())\n\n# Run profiling\nsession = obs.profile(\n    model=\"resnet50.onnx\",\n    iterations=100,\n    warmup=10,\n    lab_mode=True\n)\n\n# Analyze results\nanalysis = obs.analyze(session)\n\n# Get root cause\nroot_cause = analysis.root_cause()\nprint(f\"Bottleneck: {root_cause.category}\")\nprint(f\"Confidence: {root_cause.confidence:.2%}\")\n\n# Generate report\nobs.report(session, format=\"html\", output=\"report.html\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-workflow","title":"Example Workflow","text":""},{"location":"getting-started/quickstart/#1-profile-your-model","title":"1. Profile Your Model","text":"<pre><code>from aaco import Observatory\n\n# Initialize\nobs = Observatory()\n\n# Profile with lab mode for reproducible results\nsession = obs.profile(\n    model=\"my_model.onnx\",\n    lab_mode=True,\n    config={\n        \"iterations\": 100,\n        \"warmup\": 20,\n        \"collect_counters\": True,\n        \"collect_traces\": True\n    }\n)\n</code></pre>"},{"location":"getting-started/quickstart/#2-analyze-performance","title":"2. Analyze Performance","text":"<pre><code># Get comprehensive analysis\nanalysis = obs.analyze(session)\n\n# Hardware utilization\nheu = analysis.hardware_envelope_utilization()\nprint(f\"GPU Compute: {heu.compute:.1%}\")\nprint(f\"Memory Bandwidth: {heu.memory:.1%}\")\n\n# Bottleneck classification\nbottleneck = analysis.classify_bottleneck()\nprint(f\"Type: {bottleneck.category}\")\nprint(f\"Evidence: {bottleneck.evidence}\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-root-cause-analysis","title":"3. Root Cause Analysis","text":"<pre><code># Bayesian root cause inference\nroot_cause = analysis.root_cause()\n\nfor cause in root_cause.ranked_causes:\n    print(f\"{cause.name}: {cause.posterior:.2%}\")\n\n# Get recommendations\nfor rec in root_cause.recommendations:\n    print(f\"- {rec}\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-track-regressions","title":"4. Track Regressions","text":"<pre><code># Compare with baseline\ncomparison = obs.compare(\n    baseline=\"sessions/baseline\",\n    current=session\n)\n\n# Check for regressions\nif comparison.has_regression:\n    print(f\"Regression detected!\")\n    print(f\"  Metric: {comparison.metric}\")\n    print(f\"  Change: {comparison.change:+.2%}\")\n    print(f\"  P-value: {comparison.p_value:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Customize AACO settings</li> <li>User Guide - Deep dive into features</li> <li>API Reference - Complete API documentation</li> <li>Examples - More usage examples</li> </ul>"},{"location":"user-guide/laboratory-mode/","title":"Laboratory Mode","text":"<p>Laboratory Mode provides deterministic, reproducible profiling by controlling system variability.</p>"},{"location":"user-guide/laboratory-mode/#why-laboratory-mode","title":"Why Laboratory Mode?","text":"<p>Without isolation, measurements vary due to:</p> <ul> <li>CPU frequency scaling</li> <li>Background processes</li> <li>Memory pressure</li> <li>Thermal throttling</li> <li>Interrupt handling</li> </ul> <p>Laboratory Mode eliminates these variables for scientific measurement.</p>"},{"location":"user-guide/laboratory-mode/#quick-enable","title":"Quick Enable","text":"<pre><code># CLI\naaco profile --model model.onnx --lab-mode\n\n# Python\nsession = obs.profile(model=\"model.onnx\", lab_mode=True)\n</code></pre>"},{"location":"user-guide/laboratory-mode/#features","title":"Features","text":""},{"location":"user-guide/laboratory-mode/#cpu-isolation","title":"CPU Isolation","text":"<pre><code>laboratory:\n  cpu_isolation:\n    enabled: true\n    cores: [4, 5, 6, 7]\n    pin_threads: true\n</code></pre> <ul> <li>Pins workload to isolated CPU cores</li> <li>Prevents thread migration</li> <li>Uses <code>taskset</code> and <code>numactl</code></li> </ul>"},{"location":"user-guide/laboratory-mode/#gpu-clock-lock","title":"GPU Clock Lock","text":"<pre><code>laboratory:\n  gpu:\n    clock_lock: true\n    target_frequency: max\n</code></pre> <ul> <li>Locks GPU clock to fixed frequency</li> <li>Prevents dynamic frequency scaling</li> <li>Uses <code>rocm-smi --setperflevel</code></li> </ul>"},{"location":"user-guide/laboratory-mode/#process-isolation","title":"Process Isolation","text":"<pre><code>laboratory:\n  process:\n    nice: -20\n    cgroup_isolation: true\n</code></pre> <ul> <li>Maximum scheduling priority</li> <li>cgroups v2 resource isolation</li> <li>Memory locking (mlock)</li> </ul>"},{"location":"user-guide/laboratory-mode/#system-preparation","title":"System Preparation","text":"<pre><code>laboratory:\n  system:\n    disable_turbo: true\n    set_governor: performance\n    drop_caches: true\n</code></pre> <ul> <li>Disables CPU turbo boost</li> <li>Sets performance governor</li> <li>Clears filesystem caches</li> </ul>"},{"location":"user-guide/laboratory-mode/#requirements","title":"Requirements","text":"<p>Laboratory Mode requires:</p> Requirement Why Root/sudo CPU governor, nice values cgroups v2 Process isolation ROCm 6.0+ GPU clock control Linux 5.15+ Full eBPF support"},{"location":"user-guide/laboratory-mode/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/laboratory-mode/#basic-lab-mode","title":"Basic Lab Mode","text":"<pre><code>from aaco import Observatory, LabConfig\n\nobs = Observatory()\n\n# Use defaults\nsession = obs.profile(\n    model=\"model.onnx\",\n    lab_mode=True\n)\n</code></pre>"},{"location":"user-guide/laboratory-mode/#custom-lab-configuration","title":"Custom Lab Configuration","text":"<pre><code>lab_config = LabConfig(\n    cpu_cores=[8, 9, 10, 11],\n    gpu_clock_lock=True,\n    gpu_frequency=1800,  # MHz\n    disable_turbo=True,\n    drop_caches=True\n)\n\nsession = obs.profile(\n    model=\"model.onnx\",\n    lab_config=lab_config\n)\n</code></pre>"},{"location":"user-guide/laboratory-mode/#lab-mode-with-ebpf","title":"Lab Mode with eBPF","text":"<pre><code>session = obs.profile(\n    model=\"model.onnx\",\n    lab_mode=True,\n    ebpf=True  # Enable eBPF forensics\n)\n\n# Check interference\nsii = session.scheduler_interference_index\nprint(f\"Scheduler interference: {sii:.4f}\")\n</code></pre>"},{"location":"user-guide/laboratory-mode/#validation","title":"Validation","text":"<p>Check lab mode effectiveness:</p> <pre><code># Coefficient of Variation (CV)\ncv = session.metrics.latency.cv()\nprint(f\"CV: {cv:.2%}\")  # Should be &lt; 1%\n\n# Quality score\nquality = session.quality_score\nprint(f\"Quality: {quality}\")  # 'excellent', 'good', 'fair', 'poor'\n</code></pre> CV Quality &lt; 1% Excellent 1-3% Good 3-5% Fair &gt; 5% Poor"},{"location":"user-guide/laboratory-mode/#best-practices","title":"Best Practices","text":"<ol> <li>Warm up the system before profiling</li> <li>Run multiple sessions and compare</li> <li>Check quality scores after each run</li> <li>Document system state for reproducibility</li> </ol>"},{"location":"user-guide/laboratory-mode/#troubleshooting","title":"Troubleshooting","text":"Permission denied <p>Run with sudo or add capabilities: <pre><code>sudo aaco profile --lab-mode ...\n</code></pre></p> Clock lock not working <p>Check ROCm installation and GPU support: <pre><code>rocm-smi --showperflevel\n</code></pre></p> High CV despite lab mode <ul> <li>Check for thermal throttling</li> <li>Verify no background processes</li> <li>Try different CPU cores</li> </ul>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Welcome to the AMD AI Compute Observatory User Guide. This guide provides comprehensive documentation for all AACO features.</p>"},{"location":"user-guide/overview/#what-is-aaco","title":"What is AACO?","text":"<p>AMD AI Compute Observatory (AACO-\u03a9\u221e) is a deterministic, self-calibrating, cross-layer AI performance laboratory that provides:</p> <ul> <li>Scientific Measurement: Reproducible, statistically rigorous profiling</li> <li>Root Cause Analysis: Bayesian inference for performance bottlenecks</li> <li>Automated Governance: Statistical drift detection and regression alerts</li> <li>Hardware Optimization: Ceiling analysis with hardware digital twin</li> </ul>"},{"location":"user-guide/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/overview/#sessions","title":"Sessions","text":"<p>A session is a complete profiling run that captures:</p> <ul> <li>Execution times (warmup + measured iterations)</li> <li>Hardware counters (GPU, memory, power)</li> <li>Kernel traces (timeline, occupancy)</li> <li>System metrics (CPU, memory, thermals)</li> </ul> <pre><code>from aaco import Observatory\n\nobs = Observatory()\nsession = obs.profile(model=\"model.onnx\")\n\n# Session contains all collected data\nprint(session.summary())\n</code></pre>"},{"location":"user-guide/overview/#analysis","title":"Analysis","text":"<p>Analysis transforms raw session data into actionable insights:</p> <ul> <li>Statistical aggregation (mean, median, percentiles)</li> <li>Outlier detection and filtering</li> <li>Bottleneck classification</li> <li>Root cause inference</li> </ul> <pre><code>analysis = obs.analyze(session)\nprint(analysis.bottleneck)\nprint(analysis.root_cause())\n</code></pre>"},{"location":"user-guide/overview/#reports","title":"Reports","text":"<p>Reports present analysis results in various formats:</p> <ul> <li>HTML interactive dashboards</li> <li>JSON for programmatic access</li> <li>Markdown for documentation</li> <li>PDF for formal reports</li> </ul> <pre><code>obs.report(session, format=\"html\", output=\"report.html\")\n</code></pre>"},{"location":"user-guide/overview/#workflow","title":"Workflow","text":"<pre><code>graph LR\n    A[Model] --&gt; B[Profile]\n    B --&gt; C[Session Data]\n    C --&gt; D[Analyze]\n    D --&gt; E[Report]\n    E --&gt; F[Optimize]\n    F --&gt; A</code></pre>"},{"location":"user-guide/overview/#1-profile","title":"1. Profile","text":"<p>Run profiling to collect performance data:</p> <pre><code>aaco profile --model model.onnx --output session/\n</code></pre>"},{"location":"user-guide/overview/#2-analyze","title":"2. Analyze","text":"<p>Analyze the collected data:</p> <pre><code>aaco analyze --session session/ --output analysis.json\n</code></pre>"},{"location":"user-guide/overview/#3-report","title":"3. Report","text":"<p>Generate human-readable reports:</p> <pre><code>aaco report --session session/ --format html\n</code></pre>"},{"location":"user-guide/overview/#4-compare","title":"4. Compare","text":"<p>Compare against baselines:</p> <pre><code>aaco compare --baseline baseline/ --current session/\n</code></pre>"},{"location":"user-guide/overview/#guide-sections","title":"Guide Sections","text":"Section Description Laboratory Mode Deterministic profiling setup Profiling Data collection options Analysis Understanding metrics Dashboard Interactive visualization"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Laboratory Mode - Set up deterministic profiling</li> <li>Profiling Guide - Learn profiling options</li> <li>Examples - See practical examples</li> </ul>"}]}